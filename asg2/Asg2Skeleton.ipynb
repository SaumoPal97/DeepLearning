{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Programming Assignment 2\n",
    "--------------------------------------\n",
    "Name:Vishnu Dutt Sharma\n",
    "Roll No.: 12EC35018\n",
    "\n",
    "Submission Instructions:\n",
    "1. Fill your name and roll no in the space provided above.\n",
    "2. Name your folder in format <Roll No>_<First Name>.\n",
    "    For example 12CS10001_Rohan\n",
    "3. Submit a zipped format of the file (.zip only).\n",
    "4. Submit all your codes. But do not submit any of your datafiles\n",
    "5. From output files submit only the following 3 files. simOutput.csv, simSummary.csv, analogySolution.csv\n",
    "6. Place the three files in a folder \"output\", inside the zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "## paths to files. Do not change this\n",
    "simInputFile = \"Q1/word-similarity-dataset\"\n",
    "analogyInputFile = \"Q1/word-analogy-dataset\"\n",
    "vectorgzipFile = \"Q1/glove.6B.300d.txt.gz\"\n",
    "vectorTxtFile = \"Q1/glove.6B.300d.txt\"   # If you extract and use the gz file, use this.\n",
    "analogyTrainPath = \"Q1/wordRep/\"\n",
    "simOutputFile = \"Q1/simOutput.csv\"\n",
    "simSummaryFile = \"Q1/simSummary.csv\"\n",
    "anaSOln = \"Q1/analogySolution.csv\"\n",
    "Q4List = \"Q4/wordList.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221\n",
      "retrieved 1214\n"
     ]
    }
   ],
   "source": [
    "# Similarity Dataset\n",
    "simDataset = [item.split(\" | \") for item in open(simInputFile).read().splitlines()]\n",
    "# Analogy dataset\n",
    "analogyDataset = [[stuff.strip() for stuff in item.strip('\\n').split('\\n')] for item in open(analogyInputFile).read().split('\\n\\n')]\n",
    "\n",
    "def vectorExtract(simD = simDataset, anaD = analogyDataset, vect = vectorgzipFile):\n",
    "    simList = [stuff for item in simD for stuff in item]\n",
    "    analogyList = [thing for item in anaD for stuff in item[0:6] for thing in stuff.split()]\n",
    "    simList.extend(analogyList)\n",
    "    wordList = set(simList)\n",
    "    print len(wordList)\n",
    "    wordDict = dict()\n",
    "    \n",
    "    vectorFile = gzip.open(vect, 'r')\n",
    "    for line in vectorFile:\n",
    "        if line.split()[0].strip() in wordList:\n",
    "            wordDict[line.split()[0].strip()] = line.split()[1:]\n",
    "    \n",
    "    \n",
    "    vectorFile.close()\n",
    "    print 'retrieved', len(wordDict.keys())\n",
    "    return wordDict\n",
    "\n",
    "# Extracting Vectors from Analogy and Similarity Dataset\n",
    "validateVectors = vectorExtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of training pairs for the analogy task\n",
    "trainDict = dict()\n",
    "for subDirs in os.listdir(analogyTrainPath):\n",
    "    for files in os.listdir(analogyTrainPath+subDirs+'/'):\n",
    "        f = open(analogyTrainPath+subDirs+'/'+files).read().splitlines()\n",
    "        trainDict[files] = f\n",
    "print len(trainDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = dict()\n",
    "\n",
    "ind  =  0\n",
    "with gzip.open(vectorgzipFile, 'r') as f:\n",
    "    for line in f:\n",
    "    \n",
    "        arr = line.split(' ')\n",
    "        key = (arr[0].strip()).lower()\n",
    "        val = [float(x) for x in arr[1:]]\n",
    "        \n",
    "        word2vec[key] = val\n",
    "        ind += 1\n",
    "        \n",
    "#         if(ind % 10000 == 0):\n",
    "#             print ind\n",
    "        \n",
    "        \n",
    "f.close()\n",
    "# word2vec = pickle.load(open('Q1/word2vec.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def similarityTask(inputDS = simDataset, outputFile = simOutputFile, summaryFile=simSummaryFile, vectors=validateVectors):\n",
    "    corr_C = 0\n",
    "    corr_E = 0\n",
    "    corr_M = 0\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    mrr_C = []\n",
    "    mrr_E = []\n",
    "    mrr_M = []\n",
    "    \n",
    "    simFile = open(outputFile, 'w')\n",
    "    \n",
    "    for item in inputDS:\n",
    "        query = np.asarray(vectors[item[0]], dtype=float)\n",
    "        norm_q = np.linalg.norm(query)\n",
    "        \n",
    "        flag = 0\n",
    "        C = []\n",
    "        E = []\n",
    "        M = []\n",
    "        for option in item[1:]:\n",
    "            try:\n",
    "                opt_vec = np.asarray(vectors[option], dtype=float)\n",
    "            except:\n",
    "                flag = 1\n",
    "                break\n",
    "                \n",
    "            opt_norm = np.linalg.norm(opt_vec)\n",
    "            \n",
    "#             print len(query)\n",
    "#             print len(opt_vec)\n",
    "#             print np.inner(query, opt_vec)\n",
    "            \n",
    "            cos = np.inner(query, opt_vec)/(norm_q * opt_norm)\n",
    "            euc = np.linalg.norm(query - opt_vec)\n",
    "            man = sum(abs(query - opt_vec))\n",
    "            \n",
    "            simFile.write(str(total+1) + ',' + item[0] + ',' + option + ',' + 'C' + ',' + str(cos) + '\\n')\n",
    "            simFile.write(str(total+1) + ',' + item[0] + ',' + option + ',' + 'E' + ',' + str(euc) + '\\n')\n",
    "            simFile.write(str(total+1) + ',' + item[0] + ',' + option + ',' + 'M' + ',' + str(man) + '\\n')\n",
    "            \n",
    "            C.append(cos)\n",
    "            E.append(euc)\n",
    "            M.append(man)\n",
    "        \n",
    "        if flag:\n",
    "            flag = 0\n",
    "            continue\n",
    "        \n",
    "#         print C\n",
    "        ind = np.argmax(C)\n",
    "        if( ind == 0):\n",
    "            corr_C += 1\n",
    "        mrr_C.append(1.0/(1+np.argsort(C)[3]))\n",
    "        \n",
    "#         print E\n",
    "        ind = np.argmin(E)\n",
    "        if( ind == 0):\n",
    "            corr_E += 1\n",
    "        mrr_E.append(1.0/(1+np.argsort(E)[0]))\n",
    "        \n",
    "#         print M\n",
    "        ind = np.argmin(M)\n",
    "        if( ind == 0):\n",
    "            corr_M += 1\n",
    "        mrr_M.append(1.0/(1+np.argsort(M)[0]))\n",
    "        \n",
    "#         mrr_C.append(1.0/(1+np.argmin(np.argsort(C))))\n",
    "#         mrr_E.append(1.0/(1+np.argmin(np.argsort(E))))\n",
    "#         mrr_M.append(1.0/(1+np.argmin(np.argsort(M))))\n",
    "    \n",
    "        total += 1\n",
    "    \n",
    "    simFile.close()\n",
    "    \n",
    "    sumFile = open(summaryFile, 'w')\n",
    "    sumFile.write('C'+ ',' + str(corr_C) + ',' + str(total) + ',' + str(sum(mrr_C)/total) + '\\n')\n",
    "    sumFile.write('E'+ ',' + str(corr_E) + ',' + str(total) + ',' + str(sum(mrr_E)/total) + '\\n')\n",
    "    sumFile.write('M'+ ',' + str(corr_M) + ',' + str(total) + ',' + str(sum(mrr_M)/total) + '\\n')\n",
    "    sumFile.close()\n",
    "    \n",
    "    print corr_C, corr_E, corr_M\n",
    "    print sum(mrr_C)/total, sum(mrr_E)/total, sum(mrr_M)/total\n",
    "    \n",
    "# similarityTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_weight(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, 0, 0.001))\n",
    "#     return tf.Variable( tf.random_normal(shape, stddev=0.1) )\n",
    "\n",
    "def forward_prop(X, w1, w2):\n",
    "    hidden = tf.nn.relu( tf.matmul(X, w1) )\n",
    "\n",
    "    y_hat = tf.matmul(hidden, w2)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, train accuracy = 84.89%, test accuracy = 60.30%\n",
      "Epoch = 2, train accuracy = 90.59%, test accuracy = 79.54%\n",
      "Epoch = 3, train accuracy = 90.52%, test accuracy = 82.13%\n",
      "Epoch = 4, train accuracy = 96.11%, test accuracy = 92.22%\n",
      "Epoch = 5, train accuracy = 91.81%, test accuracy = 89.63%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24.742268041237114"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analogyTask(inputDS=analogyDataset,outputFile = anaSOln ): # add more arguments if required\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Output a file, analogySolution.csv with the following entris\n",
    "#     Query word pair, Correct option, predicted option    \n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "#     return accuracy #return the accuracy of your model after 5 fold cross validation\n",
    "\n",
    "# analogyTask()\n",
    "    RANDOM_SEED = 54\n",
    "    optn_1 = 1\n",
    "    optn_2 = 0\n",
    "\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "\n",
    "    train_X_raw = []\n",
    "    train_Y_raw = []\n",
    "\n",
    "    keys = trainDict.keys()\n",
    "    \n",
    "    \n",
    "    if optn_1:\n",
    "\n",
    "        for i in range(2000):\n",
    "            try:\n",
    "                arr = np.random.randint(19, size = 2)\n",
    "                word_1 = random.choice(trainDict[keys[arr[0]]]).split('\\t')\n",
    "                word_2 = random.choice(trainDict[keys[arr[1]]]).split('\\t')\n",
    "                train_X_raw += [[1] + word2vec[word_1[0].lower()]+ word2vec[word_1[1].lower()]+ word2vec[word_2[0].lower()]+ word2vec[word_2[1].lower()]]\n",
    "                train_Y_raw += [[1, 0]]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for i in range(2000):\n",
    "            try:\n",
    "                arr = np.random.randint(19)\n",
    "                word_1 = random.choice(trainDict[keys[arr]]).split('\\t')\n",
    "                word_2 = random.choice(trainDict[keys[arr]]).split('\\t')\n",
    "                train_X_raw += [[1] + word2vec[word_1[0].lower()] + word2vec[word_1[1].lower()] + word2vec[word_2[0].lower()] + word2vec[word_2[1].lower()]]\n",
    "                train_Y_raw += [[0, 1]]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    if optn_2:\n",
    "        flag = 0\n",
    "        for item in inputDS:\n",
    "            query = item[0].split(' ')\n",
    "\n",
    "            vec_X = []\n",
    "            for num in range(1,6):\n",
    "                option = item[num].split(' ')\n",
    "    #             test_X_raw += [[1] + word2vec[query[0].lower()] + word2vec[query[1].lower()] + word2vec[option[0].lower()] + word2vec[option[1].lower()]]\n",
    "                try:\n",
    "                    vec_X += [[1] + word2vec[query[0].lower()] + word2vec[query[1].lower()] + word2vec[option[0].lower()] + word2vec[option[1].lower()]]\n",
    "                except:\n",
    "    #                 vec_X += [[0.0] * (300*4+1)]\n",
    "                    flag = 1\n",
    "                    break\n",
    "\n",
    "            if flag:\n",
    "                flag = 0\n",
    "                continue\n",
    "\n",
    "            train_X_raw += vec_X\n",
    "\n",
    "            zer = [[1, 0]]*5\n",
    "            zer[ord(item[-1])-97] = [0, 1]\n",
    "            train_Y_raw +=  zer\n",
    "\n",
    "#     print len(train_X_raw)\n",
    "#     print len(train_Y_raw)\n",
    "#     print len(train_X_raw[0])\n",
    "\n",
    "\n",
    "    all_X = np.ndarray(shape=(len(train_X_raw), len(train_X_raw[0])))\n",
    "    all_y = np.ndarray(shape=(len(train_Y_raw), 2))\n",
    "\n",
    "#     print len(train_X_raw)\n",
    "#     print len(train_Y_raw)\n",
    "\n",
    "    index_shuf = range(len(train_Y_raw))\n",
    "    random.shuffle(index_shuf)\n",
    "    j = 0\n",
    "    for i in index_shuf:\n",
    "        all_X[j] = np.asarray(train_X_raw[i])\n",
    "        all_y[j] = np.asarray(train_Y_raw[i])\n",
    "        j += 1\n",
    "\n",
    "    train_X, test_X, train_y, test_y = train_test_split(all_X, all_y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "#     x_size = train_X.shape[1]\n",
    "#     h_size = 300                \n",
    "#     y_size = train_y.shape[1]\n",
    "\n",
    "#     X = tf.placeholder(\"float\", shape=[None, x_size])\n",
    "#     y = tf.placeholder(\"float\", shape=[None, y_size])\n",
    "\n",
    "\n",
    "#     w_1 = init_weight((x_size, h_size))\n",
    "#     w_2 = init_weight((h_size, y_size))\n",
    "\n",
    "#     y_hat = forward_prop(X, w_1, w_2)\n",
    "#     predict  = tf.nn.softmax(y_hat)\n",
    "\n",
    "#     cost    = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_hat))\n",
    "#     updates = tf.train.GradientDescentOptimizer(0.005).minimize(cost)\n",
    "\n",
    "    \n",
    "    n_hidden_1 = 300*2  # 1st layer number of features\n",
    "    n_hidden_2 = 300# 2nd layer number of features\n",
    "    n_input = train_X.shape[1] # MNIST data input (img shape: 28*28)\n",
    "    n_classes = train_y.shape[1] # MNIST total classes (0-9 digits)\n",
    "\n",
    "    \n",
    "    X = tf.placeholder(\"float\", shape=[None, n_input])\n",
    "    y = tf.placeholder(\"float\", shape=[None, n_classes])\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    \n",
    " \n",
    "    \n",
    "        # Construct model\n",
    "    predict = multilayer_perceptron(X, weights, biases)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=y))\n",
    "    updates = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        train_X, test_X, train_y, test_y = train_test_split(all_X, all_y, test_size=0.20, random_state=random.randint(1,99))\n",
    "        # Train with each example\n",
    "        for i in range(len(train_X)):\n",
    "            sess.run(updates, feed_dict={X: train_X[i: i + 1], y: train_y[i: i + 1]})\n",
    "\n",
    "        train_accuracy = np.mean(np.argmax(train_y, axis=1) ==\n",
    "                                 np.argmax(sess.run(predict, feed_dict={X: train_X, y: train_y}), axis=1))\n",
    "        test_accuracy  = np.mean(np.argmax(test_y, axis=1) ==\n",
    "                                 np.argmax(sess.run(predict, feed_dict={X: test_X, y: test_y}), axis=1))\n",
    "\n",
    "        accuracy += test_accuracy\n",
    "        print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\"\n",
    "              % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "    \n",
    "\n",
    "    accuracy = accuracy/5.0\n",
    "    \n",
    "    test_X_raw = []\n",
    "    test_Y_raw = []\n",
    "\n",
    "    \n",
    "    flag = 0\n",
    "    for item in inputDS:\n",
    "        query = item[0].split(' ')\n",
    "\n",
    "        vec_X = []\n",
    "        for num in range(1,6):\n",
    "            option = item[num].split(' ')\n",
    "#             test_X_raw += [[1] + word2vec[query[0].lower()] + word2vec[query[1].lower()] + word2vec[option[0].lower()] + word2vec[option[1].lower()]]\n",
    "            try:\n",
    "                vec_X += [[1] + word2vec[query[0].lower()] + word2vec[query[1].lower()] + word2vec[option[0].lower()] + word2vec[option[1].lower()]]\n",
    "            except:\n",
    "#                 vec_X += [[0.0] * (300*4+1)]\n",
    "                flag = 1\n",
    "                break\n",
    "        \n",
    "        if flag:\n",
    "            flag = 0\n",
    "            continue\n",
    "            \n",
    "        test_X_raw += vec_X\n",
    "\n",
    "        zer = [[1, 0]]*5\n",
    "        zer[ord(item[-1])-97] = [0, 1]\n",
    "        test_Y_raw +=  zer\n",
    "\n",
    "    test_X = np.asarray(test_X_raw)\n",
    "    test_y = np.asarray(test_Y_raw)\n",
    "    # .reshape((len(test_Y_raw),1))\n",
    "\n",
    "#     print test_X.shape\n",
    "#     print test_y.shape\n",
    "\n",
    "    out_file = open(outputFile, 'w')\n",
    "    \n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    output = sess.run( predict, feed_dict={X: test_X, y: test_y})\n",
    "#     print 'LEN',len(output)\n",
    "    for i in range(test_y.shape[0]/5):\n",
    "        out = np.argmax(output[i*5: (i+1)*5],axis=0)[1]   \n",
    "#         print output[i*5: (i+1)*5]\n",
    "#         print out, analogyDataset[i][0],chr(out+97), analogyDataset[i][-1] \n",
    "        \n",
    "        \n",
    "        out_file.write(analogyDataset[i][0])\n",
    "        out_file.write(',')\n",
    "        out_file.write(chr(out+97))\n",
    "        out_file.write(',')\n",
    "        out_file.write(analogyDataset[i][-1])\n",
    "        out_file.write('\\n')\n",
    "        \n",
    "        if( chr(out+97) == analogyDataset[i][-1]):\n",
    "            pos += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "\n",
    "    sess.close()\n",
    "    out_file.close()\n",
    "            \n",
    "#     print pos\n",
    "#     print neg\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "    return 100*float(pos)/(pos+neg)\n",
    "# analogyTask()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.85701067]), array([ 0.31210075]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def derivedWordTask(inputFile = Q4List):\n",
    "    fasttext_dict = dict()\n",
    "    with open('Q4/fastText_vectors.txt', 'r') as f:\n",
    "        for line in f:\n",
    "\n",
    "            arr = line.strip().split(' ')\n",
    "            key = (arr[0].strip()).lower()\n",
    "            val = [float(x) for x in arr[1:]]\n",
    "\n",
    "            fasttext_dict[key] = val\n",
    "\n",
    "    lazaridou_dict = dict()\n",
    "    with open('Q4/vector_lazaridou.txt', 'r') as f:\n",
    "        for line in f:\n",
    "\n",
    "            arr = line.strip().split('[')\n",
    "            key = (arr[0].strip()).lower()\n",
    "            arr = (arr[1].strip()[:-1]).split(',')\n",
    "            val = [float(x) for x in arr[1:]]\n",
    "\n",
    "            lazaridou_dict[key] = val\n",
    "\n",
    "    index = []\n",
    "    affix = []\n",
    "    derived = []\n",
    "    root = []\n",
    "\n",
    "    csvfile = open(inputFile, 'rb')\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    header = spamreader.next()\n",
    "    for row in spamreader:\n",
    "        index.append(int(row[0]))\n",
    "        affix.append(row[1])\n",
    "        derived.append(row[2])\n",
    "        root.append(row[3])\n",
    "    csvfile.close()\n",
    "\n",
    "    name = list(set(affix))\n",
    "    aff_dict = dict(zip(name, range(len(name))))\n",
    "\n",
    "    h_size  = 300\n",
    "\n",
    "    train_X_ft = []\n",
    "    train_Y_ft = []\n",
    "    for i in range(len(root)):\n",
    "        try:\n",
    "            zer = [0]*len(name)\n",
    "            zer[aff_dict[affix[i]]] = 1\n",
    "            train_X_ft += [[1]+fasttext_dict[root[i]] + zer]\n",
    "            train_Y_ft += [fasttext_dict[derived[i]]]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    train_X_lz = []\n",
    "    train_Y_lz = []                      \n",
    "    for i in range(len(root)):\n",
    "        try:\n",
    "            zer = [0]*len(name)\n",
    "            zer[aff_dict[affix[i]]] = 1\n",
    "            train_X_lz += [[1]+lazaridou_dict[root[i]] + zer]\n",
    "            train_Y_lz += [lazaridou_dict[derived[i]]]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "#     train_X_ft = np.asarray(train_X_ft)\n",
    "#     train_Y_ft = np.asarray(train_Y_ft)\n",
    "#     train_X_lz = np.asarray(train_X_lz)\n",
    "#     train_Y_lz = np.asarray(train_Y_lz)\n",
    "    \n",
    "    \n",
    "    x_size_ft = len(train_X_ft[0])\n",
    "    h_size_ft = 300                \n",
    "    y_size_ft = len(train_Y_ft[0])\n",
    "\n",
    "    x_size_lz = len(train_X_lz[0])\n",
    "    h_size_lz = 400        \n",
    "    y_size_lz = len(train_Y_lz[0])\n",
    "\n",
    "    \n",
    "    X_ft = tf.placeholder(\"float\", shape=[None, x_size_ft])\n",
    "    y_ft = tf.placeholder(\"float\", shape=[None, y_size_ft])\n",
    "    X_lz = tf.placeholder(\"float\", shape=[None, x_size_lz])\n",
    "    y_lz = tf.placeholder(\"float\", shape=[None, y_size_lz])\n",
    "    \n",
    "\n",
    "    w_ft_1 = init_weight((x_size_ft, h_size_ft))\n",
    "    w_ft_2 = init_weight((h_size_ft, y_size_ft))\n",
    "    w_lz_1 = init_weight((x_size_lz, h_size_lz))\n",
    "    w_lz_2 = init_weight((h_size_lz, y_size_lz))\n",
    "\n",
    "    \n",
    "    y_hat_ft = forward_prop(X_ft, w_ft_1, w_ft_2)\n",
    "    predict_ft = y_hat_ft\n",
    "    y_hat_lz = forward_prop(X_lz, w_lz_1, w_lz_2)\n",
    "    predict_lz = y_hat_lz\n",
    "\n",
    "\n",
    "\n",
    "    cost_ft = tf.reduce_mean(tf.nn.l2_loss(y_ft - y_hat_ft))\n",
    "#     cost_ft = tf.reduce_mean(tf.squared_difference(y_ft, y_hat_ft))\n",
    "#     cost_ft = tf.reduce_mean(tf.square(y_ft - y_hat_ft))\n",
    "    updates_ft = tf.train.GradientDescentOptimizer(0.01).minimize(cost_ft)\n",
    "    \n",
    "    cost_lz = tf.reduce_mean(tf.nn.l2_loss(y_lz - y_hat_lz))\n",
    "#     cost_lz = tf.reduce_mean(tf.squared_difference(y_lz, y_hat_lz))\n",
    "#     cost_lz = tf.reduce_mean(tf.square(y_lz - y_hat_lz))\n",
    "    updates_lz = tf.train.AdamOptimizer(0.001).minimize(cost_lz)\n",
    "\n",
    "\n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "       \n",
    "        \n",
    "    for epoch in range(5):\n",
    "        train_X, test_X, train_y, test_y = train_test_split(train_X_ft, train_Y_ft, test_size=0.20, random_state=random.randint(1,99))\n",
    "        # Train with each example\n",
    "        for i in range(len(train_X)):\n",
    "            sess.run(updates_ft, feed_dict={X_ft: train_X[i: i + 1], y_ft: train_y[i: i + 1]})\n",
    "            \n",
    "#         print len(train_y)\n",
    "#         print len(train_y[0])\n",
    "#         print np.mean(np.square(train_y - sess.run(predict_ft, feed_dict={X_ft: train_X, y_ft: train_y}))).shape\n",
    "        \n",
    "#         train_accuracy = np.mean(np.square(train_y - sess.run(predict_ft, feed_dict={X_ft: train_X, y_ft: train_y})))\n",
    "#         test_accuracy  = np.mean(np.square(test_y - sess.run(predict_ft, feed_dict={X_ft: test_X, y_ft: test_y}) ))\n",
    "\n",
    "      \n",
    "        print \"FT: Epoch = \",epoch + 1\n",
    "#         print(\"FT: Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\"\n",
    "#               % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        train_X, test_X, train_y, test_y = train_test_split(train_X_lz, train_Y_lz, test_size=0.20, random_state=random.randint(1,99))\n",
    "        # Train with each example\n",
    "        \n",
    "        for i in range(len(train_X)):\n",
    "            sess.run(updates_lz, feed_dict={X_lz: train_X[i: i + 1], y_lz: train_y[i: i + 1]})\n",
    "            \n",
    "#         train_accuracy = np.mean(np.square(train_y - sess.run(predict_lz, feed_dict={X_lz: train_X, y_lz: train_y})))\n",
    "#         test_accuracy  = np.mean(np.square(test_y - sess.run(predict_lz, feed_dict={X_lz: test_X, y_lz: test_y}) ))\n",
    "\n",
    "        print \"LZ: Epoch = \",epoch + 1\n",
    "#         print(\"LZ: Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\"\n",
    "#               % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "        \n",
    "    \n",
    "    ft_file = open('Q4/AnsFastText.txt', 'w')\n",
    "    lz_file = open('Q4/AnsLzaridou.txt', 'w')\n",
    "    mod_ft_file = open('Q4/AnsModel.txt', 'w')\n",
    "#     mod_lz_file = open('Q4/AnsModelLZ.txt', 'w')\n",
    "\n",
    "    ft_c = 0.0\n",
    "    lz_c = 0.0\n",
    "    tot = 0\n",
    "    for i in range(len(root)):\n",
    "        ft_file.write(derived[i])\n",
    "        out_1_ft = fasttext_dict[derived[i]]\n",
    "        for k in out_1_ft:\n",
    "            ft_file.write(' '+str(k))\n",
    "        ft_file.write('\\n')\n",
    "\n",
    "        lz_file.write(derived[i])\n",
    "        out_1_lz = lazaridou_dict[derived[i]]\n",
    "        for k in out_1_lz:\n",
    "            lz_file.write(' '+str(k))\n",
    "        lz_file.write('\\n')\n",
    "\n",
    "        zer = [0]*len(name)\n",
    "        zer[aff_dict[affix[i]]] = 1\n",
    "        train_X = [[1]+fasttext_dict[root[i]] + zer]\n",
    "\n",
    "        mod_ft_file.write(derived[i])\n",
    "\n",
    "        out_2_ft = sess.run( predict_ft, feed_dict={X_ft: train_X})\n",
    "\n",
    "        for k in out_2_ft:\n",
    "            mod_ft_file.write(' '+str(k))\n",
    "        mod_ft_file.write('\\n')\n",
    "\n",
    "        zer = [0]*len(name)\n",
    "        zer[aff_dict[affix[i]]] = 1\n",
    "        train_X = [[1]+lazaridou_dict[root[i]] + zer]\n",
    "\n",
    "\n",
    "        out_2_lz = sess.run( predict_lz, feed_dict={X_lz: train_X})\n",
    "\n",
    "        norm1 = np.linalg.norm(np.asarray(out_1_ft))\n",
    "        norm2 = np.linalg.norm(out_2_ft)\n",
    "        if(norm1 < 0.001 or norm2 < 0.001):\n",
    "            cos_ft = 0.0\n",
    "        else:\n",
    "            cos_ft = np.inner(np.asarray(out_1_ft), out_2_ft)/(norm1 * norm2)\n",
    "        \n",
    "        norm1 = np.linalg.norm(np.asarray(out_1_lz))\n",
    "        norm2 = np.linalg.norm(out_2_lz)\n",
    "        if(norm1 < 0.001 or norm2 < 0.001):\n",
    "            cos_lz = 0.0\n",
    "        else:\n",
    "            cos_lz = np.inner(np.asarray(out_1_lz), out_2_lz)/(norm1 * norm2)\n",
    "\n",
    "        ft_c += cos_ft\n",
    "        lz_c += cos_lz\n",
    "        tot += 1\n",
    "\n",
    "\n",
    "    ft_file.close()\n",
    "    lz_file.close()\n",
    "    mod_ft_file.close()\n",
    "\n",
    "    cosVal1 = ft_c/tot\n",
    "    cosVal2 = lz_c/tot\n",
    "\n",
    "    return cosVal1, cosVal2\n",
    "# derivedWordTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 17 17\n",
      "0.770833333333 0.671296296296 0.662037037037\n",
      "Epoch = 1, train accuracy = 85.24%, test accuracy = 61.70%\n",
      "Epoch = 2, train accuracy = 91.54%, test accuracy = 77.70%\n",
      "Epoch = 3, train accuracy = 92.30%, test accuracy = 85.09%\n",
      "Epoch = 4, train accuracy = 95.93%, test accuracy = 92.89%\n",
      "Epoch = 5, train accuracy = 94.52%, test accuracy = 92.75%\n",
      "FT: Epoch =  1\n",
      "FT: Epoch =  2\n",
      "FT: Epoch =  3\n",
      "FT: Epoch =  4\n",
      "FT: Epoch =  5\n",
      "LZ: Epoch =  1\n",
      "LZ: Epoch =  2\n",
      "LZ: Epoch =  3\n",
      "LZ: Epoch =  4\n",
      "LZ: Epoch =  5\n",
      "18.5567010309\n",
      "[ 0.85518842] [ 0.32187445]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    similarityTask()\n",
    "    anaSim = analogyTask()\n",
    "    derCos1,derCos2 = derivedWordTask()\n",
    "    print anaSim\n",
    "    print derCos1, derCos2\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# word2vec = pickle.load(open('Q1/word2vec.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def derivedWordTask(inputFile = Q4List):\n",
    "#     fasttext_dict = dict()\n",
    "#     with open('Q4/fastText_vectors.txt', 'r') as f:\n",
    "#         for line in f:\n",
    "\n",
    "#             arr = line.strip().split(' ')\n",
    "#             key = (arr[0].strip()).lower()\n",
    "#             val = [float(x) for x in arr[1:]]\n",
    "\n",
    "#             fasttext_dict[key] = val\n",
    "\n",
    "#     lazaridou_dict = dict()\n",
    "#     with open('Q4/vector_lazaridou.txt', 'r') as f:\n",
    "#         for line in f:\n",
    "\n",
    "#             arr = line.strip().split('[')\n",
    "#             key = (arr[0].strip()).lower()\n",
    "#             arr = (arr[1].strip()[:-1]).split(',')\n",
    "#             val = [float(x) for x in arr[1:]]\n",
    "\n",
    "#             lazaridou_dict[key] = val\n",
    "\n",
    "#     index = []\n",
    "#     affix = []\n",
    "#     derived = []\n",
    "#     root = []\n",
    "\n",
    "#     csvfile = open(Q4List, 'rb')\n",
    "#     spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "#     header = spamreader.next()\n",
    "#     for row in spamreader:\n",
    "#         index.append(int(row[0]))\n",
    "#         affix.append(row[1])\n",
    "#         derived.append(row[2])\n",
    "#         root.append(row[3])\n",
    "#     csvfile.close()\n",
    "\n",
    "#     name = list(set(affix))\n",
    "#     aff_dict = dict(zip(name, range(len(name))))\n",
    "\n",
    "#     h_size  = 300\n",
    "\n",
    "#     train_X_raw_ft = []\n",
    "#     train_Y_raw_ft = []\n",
    "#     train_X_ft = []\n",
    "#     train_Y_ft = []\n",
    "#     for i in range(len(root)):\n",
    "#         try:\n",
    "#             zer = [0]*len(name)\n",
    "#             zer[aff_dict[affix[i]]] = 1\n",
    "#             train_X_raw_ft += [[1]+ fasttext_dict[root[i]] + zer]\n",
    "#             train_Y_raw_ft += [fasttext_dict[derived[i]]]\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "#     index_shuf = range(len(train_Y_raw_ft))\n",
    "#     random.shuffle(index_shuf)\n",
    "#     for i in index_shuf:\n",
    "#         train_X_ft.append(train_X_raw_ft[i])\n",
    "#         train_Y_ft.append(train_Y_raw_ft[i])\n",
    "\n",
    "#     train_X_raw_lz = []\n",
    "#     train_Y_raw_lz = []\n",
    "#     train_X_lz = []\n",
    "#     train_Y_lz = []                      \n",
    "#     for i in range(len(root)):\n",
    "#         try:\n",
    "#             zer = [0]*len(name)\n",
    "#             zer[aff_dict[affix[i]]] = 1\n",
    "#             train_X_raw_lz += [[1]+ lazaridou_dict[root[i]] + zer]\n",
    "#             train_Y_raw_lz += [lazaridou_dict[derived[i]]]\n",
    "#         except:\n",
    "#             continue\n",
    "\n",
    "#     index_shuf = range(len(train_Y_raw_lz))\n",
    "#     random.shuffle(index_shuf)\n",
    "#     for i in index_shuf:\n",
    "#         train_X_lz.append(train_X_raw_lz[i])\n",
    "#         train_Y_lz.append(train_Y_raw_lz[i])\n",
    "\n",
    "#     kf = KFold(n_splits=5)\n",
    "#     for train_index, test_index in kf.split(train_X_ft):\n",
    "#     #         print len(train_index)\n",
    "#     #         print len(test_index)\n",
    "#             X_train, X_test = np.asarray(train_X_ft)[train_index], np.asarray(train_X_ft)[test_index]\n",
    "#             Y_train, Y_test = np.asarray(train_Y_ft)[train_index], np.asarray(train_Y_ft)[test_index]\n",
    "#     #         print X_train[1:10]\n",
    "#     #         print X_test[1:10]\n",
    "\n",
    "\n",
    "#             mlp_ft = MLPRegressor(solver='lbfgs', hidden_layer_sizes=300, max_iter=150, shuffle=True, random_state=1, activation='relu')\n",
    "#             mlp_ft.fit(X_train, Y_train)\n",
    "#             score = mlp_ft.score(X_test, Y_test)\n",
    "# #             print 'Score ft: ',score\n",
    "\n",
    "\n",
    "#     for train_index, test_index in kf.split(train_X_lz):\n",
    "#     #         print len(train_index)\n",
    "#     #         print len(test_index)\n",
    "#             X_train, X_test = np.asarray(train_X_lz)[train_index], np.asarray(train_X_lz)[test_index]\n",
    "#             Y_train, Y_test = np.asarray(train_Y_lz)[train_index], np.asarray(train_Y_lz)[test_index]\n",
    "#     #         print X_train[1:10]\n",
    "#     #         print X_test[1:10]\n",
    "\n",
    "\n",
    "#             mlp_lz = MLPRegressor(solver='lbfgs', hidden_layer_sizes=450, max_iter=150, shuffle=True, random_state=1, activation='relu')\n",
    "#             mlp_lz.fit(X_train, Y_train)\n",
    "#             score = mlp_lz.score(X_test, Y_test)\n",
    "# #             print 'Score lz: ',score\n",
    "\n",
    "\n",
    "\n",
    "#     ft_file = open('Q4/AnsFastText.txt', 'w')\n",
    "#     lz_file = open('Q4/AnsLzaridou.txt', 'w')\n",
    "#     mod_ft_file = open('Q4/AnsModel.txt', 'w')\n",
    "# #     mod_lz_file = open('Q4/AnsModelLZ.txt', 'w')\n",
    "\n",
    "#     ft_c = 0.0\n",
    "#     lz_c = 0.0\n",
    "#     tot = 0\n",
    "#     for i in range(len(root)):\n",
    "#         ft_file.write(derived[i])\n",
    "#         out_1_ft = fasttext_dict[derived[i]]\n",
    "#         for k in out_1_ft:\n",
    "#             ft_file.write(' '+str(k))\n",
    "#         ft_file.write('\\n')\n",
    "\n",
    "#         lz_file.write(derived[i])\n",
    "#         out_1_lz = lazaridou_dict[derived[i]]\n",
    "#         for k in out_1_lz:\n",
    "#             lz_file.write(' '+str(k))\n",
    "#         lz_file.write('\\n')\n",
    "\n",
    "#         zer = [0]*len(name)\n",
    "#         zer[aff_dict[affix[i]]] = 1\n",
    "#         train_X = [[1]+ fasttext_dict[root[i]] + zer]\n",
    "\n",
    "#         mod_ft_file.write(derived[i])\n",
    "\n",
    "#         out_2_ft = mlp_ft.predict(train_X)\n",
    "\n",
    "#         for k in out_2_ft:\n",
    "#             mod_ft_file.write(' '+str(k))\n",
    "#         mod_ft_file.write('\\n')\n",
    "\n",
    "#         zer = [0]*len(name)\n",
    "#         zer[aff_dict[affix[i]]] = 1\n",
    "#         train_X = [[1]+ lazaridou_dict[root[i]] + zer]\n",
    "\n",
    "\n",
    "#         out_2_lz = mlp_lz.predict(train_X)\n",
    "\n",
    "#         cos_ft = np.inner(np.asarray(out_1_ft), out_2_ft)/(np.linalg.norm(np.asarray(out_1_ft)) * np.linalg.norm(out_2_ft))\n",
    "#         cos_lz = np.inner(np.asarray(out_1_lz), out_2_lz)/(np.linalg.norm(np.asarray(out_1_lz)) * np.linalg.norm(out_2_lz))\n",
    "\n",
    "#         ft_c += cos_ft\n",
    "#         lz_c += cos_lz\n",
    "#         tot += 1\n",
    "\n",
    "\n",
    "#     ft_file.close()\n",
    "#     lz_file.close()\n",
    "#     mod_ft_file.close()\n",
    "\n",
    "#     cosVal1 = ft_c/tot\n",
    "#     cosVal2 = lz_c/tot\n",
    "\n",
    "#     return cosVal1, cosVal2\n",
    "# # derivedWordTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
