{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Programming Assignment 2\n",
    "--------------------------------------\n",
    "Name:Vishnu Dutt Sharma\n",
    "Roll No.: 12EC35018\n",
    "\n",
    "Submission Instructions:\n",
    "1. Fill your name and roll no in the space provided above.\n",
    "2. Name your folder in format <Roll No>_<First Name>.\n",
    "    For example 12CS10001_Rohan\n",
    "3. Submit a zipped format of the file (.zip only).\n",
    "4. Submit all your codes. But do not submit any of your datafiles\n",
    "5. From output files submit only the following 3 files. simOutput.csv, simSummary.csv, analogySolution.csv\n",
    "6. Place the three files in a folder \"output\", inside the zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "## paths to files. Do not change this\n",
    "simInputFile = \"Q1/word-similarity-dataset\"\n",
    "analogyInputFile = \"Q1/word-analogy-dataset\"\n",
    "vectorgzipFile = \"Q1/glove.6B.300d.txt.gz\"\n",
    "vectorTxtFile = \"Q1/glove.6B.300d.txt\"   # If you extract and use the gz file, use this.\n",
    "analogyTrainPath = \"Q1/wordRep/\"\n",
    "simOutputFile = \"Q1/simOutput.csv\"\n",
    "simSummaryFile = \"Q1/simSummary.csv\"\n",
    "anaSOln = \"Q1/analogySolution.csv\"\n",
    "Q4List = \"Q4/wordList.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221\n",
      "retrieved 1214\n"
     ]
    }
   ],
   "source": [
    "# Similarity Dataset\n",
    "simDataset = [item.split(\" | \") for item in open(simInputFile).read().splitlines()]\n",
    "# Analogy dataset\n",
    "analogyDataset = [[stuff.strip() for stuff in item.strip('\\n').split('\\n')] for item in open(analogyInputFile).read().split('\\n\\n')]\n",
    "\n",
    "def vectorExtract(simD = simDataset, anaD = analogyDataset, vect = vectorgzipFile):\n",
    "    simList = [stuff for item in simD for stuff in item]\n",
    "    analogyList = [thing for item in anaD for stuff in item[0:6] for thing in stuff.split()]\n",
    "    simList.extend(analogyList)\n",
    "    wordList = set(simList)\n",
    "    print len(wordList)\n",
    "    wordDict = dict()\n",
    "    \n",
    "    vectorFile = gzip.open(vect, 'r')\n",
    "    for line in vectorFile:\n",
    "        if line.split()[0].strip() in wordList:\n",
    "            wordDict[line.split()[0].strip()] = line.split()[1:]\n",
    "    \n",
    "    \n",
    "    vectorFile.close()\n",
    "    print 'retrieved', len(wordDict.keys())\n",
    "    return wordDict\n",
    "\n",
    "# Extracting Vectors from Analogy and Similarity Dataset\n",
    "validateVectors = vectorExtract()\n",
    "# print validateVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of training pairs for the analogy task\n",
    "trainDict = dict()\n",
    "for subDirs in os.listdir(analogyTrainPath):\n",
    "    for files in os.listdir(analogyTrainPath+subDirs+'/'):\n",
    "        f = open(analogyTrainPath+subDirs+'/'+files).read().splitlines()\n",
    "        trainDict[files] = f\n",
    "print len(trainDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vec = dict()\n",
    "\n",
    "with gzip.open(vectorgzipFile, 'r') as fl:\n",
    "    for lines in fl:\n",
    "        vec = lines.split(' ')\n",
    "        key = (vec[0].strip()).lower()\n",
    "        val = [float(x) for x in arr[1:]]\n",
    "        \n",
    "        word_vec[key] = val\n",
    "    \n",
    "fl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 17 17\n",
      "0.770833333333 0.671296296296 0.662037037037\n"
     ]
    }
   ],
   "source": [
    "def similarityTask(inputDS = simDataset, outputFile = simOutputFile, summaryFile=simSummaryFile, vectors=validateVectors):\n",
    "    corr_cos = 0\n",
    "    corr_euc = 0\n",
    "    corr_man = 0\n",
    "    mrr_cos = []\n",
    "    mrr_euc = []\n",
    "    mrr_man = []\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    sim_fl = open(simOutputFile, 'w')\n",
    "    for line in simDataset:\n",
    "        query = np.asarray(validateVectors[line[0]], dtype=float)\n",
    "                \n",
    "        flag = 0\n",
    "        \n",
    "        cos_vec = []\n",
    "        euc_vec = []\n",
    "        man_vec = []\n",
    "        \n",
    "        for word in line[1:]:\n",
    "            try:\n",
    "                opt_vec = np.asarray(validateVectors[word], dtype=float)\n",
    "            except:\n",
    "                flag = 1\n",
    "                break\n",
    "                \n",
    "            cos = np.inner(query, opt_vec)/(np.linalg.norm(query) * np.linalg.norm(opt_vec))\n",
    "            sim_fl.write(\"%d,%s,%s,C,%f\\n\" % (index+1, line[0], word, cos))\n",
    "            \n",
    "            euc = np.linalg.norm(query - opt_vec)\n",
    "            sim_fl.write(\"%d,%s,%s,E,%f\\n\" % (index+1, line[0], word, euc))\n",
    "            \n",
    "            man = sum(abs(query - opt_vec))\n",
    "            sim_fl.write(\"%d,%s,%s,M,%f\\n\" % (index+1, line[0], word, man))\n",
    "            \n",
    "            cos_vec += [cos]\n",
    "            euc_vec += [euc]\n",
    "            man_vec += [man]\n",
    "            \n",
    "        if flag:\n",
    "            flag = 0\n",
    "            continue\n",
    "        \n",
    "        index = index + 1\n",
    "        \n",
    "        corr = np.argmax(cos_vec)\n",
    "        if( corr == 0):\n",
    "            corr_cos += 1\n",
    "        \n",
    "        corr = np.argmin(euc_vec)\n",
    "        if( corr == 0):\n",
    "            corr_euc += 1\n",
    "        \n",
    "        corr = np.argmin(man_vec)\n",
    "        if( corr == 0):\n",
    "            corr_man += 1\n",
    "        \n",
    "        mrr_cos.append(1.0/(1+np.argsort(cos_vec)[3]))\n",
    "        mrr_euc.append(1.0/(1+np.argsort(euc_vec)[0]))\n",
    "        mrr_man.append(1.0/(1+np.argsort(man_vec)[0]))\n",
    "\n",
    "    sim_fl.close()\n",
    "    \n",
    "    mrr_C = sum(mrr_cos)/len(mrr_cos)\n",
    "    mrr_E = sum(mrr_euc)/len(mrr_euc)\n",
    "    mrr_M = sum(mrr_man)/len(mrr_man)\n",
    "    \n",
    "    sum_fl = open(simSummaryFile, 'w')\n",
    "    sum_fl.write(\"C,%d,%d,%f\\n\" % (corr_cos, len(mrr_cos), mrr_C))\n",
    "    sum_fl.write(\"E,%d,%d,%f\\n\" % (corr_euc, len(mrr_cos), mrr_E))\n",
    "    sum_fl.write(\"M,%d,%d,%f\\n\" % (corr_man, len(mrr_cos), mrr_M))\n",
    "    sum_fl.close()\n",
    "\n",
    "\n",
    "# similarityTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analogyTask(inputDS=analogyDataset,outputFile = anaSOln ): # add more arguments if required\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "\n",
    "    hid_size = 300\n",
    "\n",
    "    keys = trainDict.keys()\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    while(i < 5000):\n",
    "        try:\n",
    "            index = np.random.randint(19, size = 2)\n",
    "            \n",
    "            word_1 = random.choice(trainDict[keys[index[0]]]).split('\\t')\n",
    "            word_2 = random.choice(trainDict[keys[index[1]]]).split('\\t')\n",
    "            \n",
    "            train_Y += [0]\n",
    "            train_X += [word2vec[word_1[0].lower()]+ word2vec[word_1[1].lower()]+ word2vec[word_2[0].lower()]+ word2vec[word_2[1].lower()]]\n",
    "            \n",
    "            index = np.random.randint(13)\n",
    "            \n",
    "            word_1 = random.choice(trainDict[keys[index]]).split('\\t')\n",
    "            word_2 = random.choice(trainDict[keys[index]]).split('\\t')\n",
    "            \n",
    "            train_Y += [1]\n",
    "            train_X += [word2vec[word_1[0].lower()] + word2vec[word_1[1].lower()] + word2vec[word_2[0].lower()] + word2vec[word_2[1].lower()]]\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "            \n",
    "    accuracy = 0.0\n",
    "\n",
    "    kf = KFold(n_splits=5)\n",
    "    epoch = 0\n",
    "    for train_index, test_index in kf.split(train_X):\n",
    "        X_train, X_test = np.asarray(train_X)[train_index], np.asarray(train_X)[test_index]\n",
    "        Y_train, Y_test = np.asarray(train_Y)[train_index], np.asarray(train_Y)[test_index]\n",
    "\n",
    "        nn = MLPClassifier(solver='lbfgs', alpha=2e-5, hidden_layer_sizes=(hid_size, 1), random_state=1)\n",
    "        nn.fit(X_train, Y_train)\n",
    "        \n",
    "        acc = clf.score(X_test, Y_test)\n",
    "        \n",
    "        print 'Epoch: ', epoch+1, ', Acccuracy: ', accuracy, '%'\n",
    "        \n",
    "        accuracy += acc\n",
    "        epoch += 1\n",
    "\n",
    "    acc = acc/5.0\n",
    "    print acc\n",
    "\n",
    "\n",
    "    \n",
    "    ana_fl = open(anaSOln, 'w')\n",
    "\n",
    "    inco = 0\n",
    "    corr = 0\n",
    "    flag = 0\n",
    "\n",
    "\n",
    "    for line in analogyDataset:\n",
    "        ques = (line[0]).split(' ')\n",
    "        \n",
    "        X_vec = []\n",
    "        Y_vec = []\n",
    "\n",
    "        for ind in range(1,6):\n",
    "            option = line[ind].split(' ')\n",
    "            try:\n",
    "                X_vec += [word2vec[query[0].lower()] + word2vec[query[1].lower()] + word2vec[option[0].lower()] + word2vec[option[1].lower()]]\n",
    "            except:\n",
    "                flag = 1\n",
    "\n",
    "        if flag:\n",
    "            flag = 0\n",
    "            continue\n",
    "\n",
    "        prob = nn.predict_proba(test_X)[1]\n",
    "        y_hat = np.argmax(prob)\n",
    "        y = ord(line[-1])-97\n",
    "\n",
    "        anaOutFile.write(item[0] + ',' + item[-1] + ',' + chr(output+97) + '\\n')\n",
    "        \n",
    "        if (y_hat == y):\n",
    "                corr += 1\n",
    "        else:\n",
    "                inco += 1\n",
    "\n",
    "    anaOutFile.close()        \n",
    "\n",
    "    return float(corr)/(inco+corr)\n",
    "\n",
    "# analogyTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def derivedWordTask(inputFile = Q4List):\n",
    "#     print 'hello world'\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Output vectors of 3 files:\n",
    "    1)AnsFastText.txt - fastText vectors of derived words in wordList.csv\n",
    "    2)AnsLzaridou.txt - Lazaridou vectors of the derived words in wordList.csv\n",
    "    3)AnsModel.txt - Vectors for derived words as provided by the model\n",
    "    \n",
    "    For all the three files, each line should contain a derived word and its vector, exactly like \n",
    "    the format followed in \"glove.6B.300d.txt\"\n",
    "    \n",
    "    word<space>dim1<space>dim2........<space>dimN\n",
    "    charitably 256.238 0.875 ...... 1.234\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    The function should return 2 values\n",
    "    1) Averaged cosine similarity between the corresponding words from output files 1 and 3, as well as 2 and 3.\n",
    "    \n",
    "        - if there are 3 derived words in wordList.csv, say word1, word2, word3\n",
    "        then find the cosine similiryt between word1 in AnsFastText.txt and word1 in AnsModel.txt.\n",
    "        - Repeat the same for word2 and word3.\n",
    "        - Average the 3 cosine similarity values\n",
    "        - DO the same for word1 to word3 between the files AnsLzaridou.txt and AnsModel.txt \n",
    "        and average the cosine simialities for valuse so obtained\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    fasttext_dict = dict()\n",
    "    with open('Q4/fastText_vectors.txt', 'r') as f:\n",
    "        for line in f:\n",
    "\n",
    "            arr = line.strip().split(' ')\n",
    "            key = (arr[0].strip()).lower()\n",
    "            val = [float(x) for x in arr[1:]]\n",
    "\n",
    "            fasttext_dict[key] = val\n",
    "\n",
    "    lazaridou_dict = dict()\n",
    "    with open('Q4/vector_lazaridou.txt', 'r') as f:\n",
    "        for line in f:\n",
    "\n",
    "            arr = line.strip().split('[')\n",
    "            key = (arr[0].strip()).lower()\n",
    "            arr = (arr[1].strip()[:-1]).split(',')\n",
    "            val = [float(x) for x in arr[1:]]\n",
    "\n",
    "            lazaridou_dict[key] = val\n",
    "\n",
    "    index = []\n",
    "    affix = []\n",
    "    derived = []\n",
    "    root = []\n",
    "\n",
    "    csvfile = open(Q4List, 'rb')\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    header = spamreader.next()\n",
    "    for row in spamreader:\n",
    "        index.append(int(row[0]))\n",
    "        affix.append(row[1])\n",
    "        derived.append(row[2])\n",
    "        root.append(row[3])\n",
    "    csvfile.close()\n",
    "\n",
    "    name = list(set(affix))\n",
    "    aff_dict = dict(zip(name, range(len(name))))\n",
    "\n",
    "    h_size  = 300\n",
    "\n",
    "    train_X_raw_ft = []\n",
    "    train_Y_raw_ft = []\n",
    "    train_X_ft = []\n",
    "    train_Y_ft = []\n",
    "    for i in range(len(root)):\n",
    "        try:\n",
    "            zer = [0]*len(name)\n",
    "            zer[aff_dict[affix[i]]] = 1\n",
    "            train_X_raw_ft += [[1]+ fasttext_dict[root[i]] + zer]\n",
    "            train_Y_raw_ft += [fasttext_dict[derived[i]]]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    index_shuf = range(len(train_Y_raw_ft))\n",
    "    random.shuffle(index_shuf)\n",
    "    for i in index_shuf:\n",
    "        train_X_ft.append(train_X_raw_ft[i])\n",
    "        train_Y_ft.append(train_Y_raw_ft[i])\n",
    "\n",
    "    train_X_raw_lz = []\n",
    "    train_Y_raw_lz = []\n",
    "    train_X_lz = []\n",
    "    train_Y_lz = []                      \n",
    "    for i in range(len(root)):\n",
    "        try:\n",
    "            zer = [0]*len(name)\n",
    "            zer[aff_dict[affix[i]]] = 1\n",
    "            train_X_raw_lz += [[1]+ lazaridou_dict[root[i]] + zer]\n",
    "            train_Y_raw_lz += [lazaridou_dict[derived[i]]]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    index_shuf = range(len(train_Y_raw_lz))\n",
    "    random.shuffle(index_shuf)\n",
    "    for i in index_shuf:\n",
    "        train_X_lz.append(train_X_raw_lz[i])\n",
    "        train_Y_lz.append(train_Y_raw_lz[i])\n",
    "\n",
    "    kf = KFold(n_splits=5)\n",
    "    for train_index, test_index in kf.split(train_X_ft):\n",
    "    #         print len(train_index)\n",
    "    #         print len(test_index)\n",
    "            X_train, X_test = np.asarray(train_X_ft)[train_index], np.asarray(train_X_ft)[test_index]\n",
    "            Y_train, Y_test = np.asarray(train_Y_ft)[train_index], np.asarray(train_Y_ft)[test_index]\n",
    "    #         print X_train[1:10]\n",
    "    #         print X_test[1:10]\n",
    "\n",
    "\n",
    "            mlp_ft = MLPRegressor(solver='lbfgs', hidden_layer_sizes=300, max_iter=150, shuffle=True, random_state=1, activation='relu')\n",
    "            mlp_ft.fit(X_train, Y_train)\n",
    "            score = mlp_ft.score(X_test, Y_test)\n",
    "#             print 'Score ft: ',score\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(train_X_lz):\n",
    "    #         print len(train_index)\n",
    "    #         print len(test_index)\n",
    "            X_train, X_test = np.asarray(train_X_lz)[train_index], np.asarray(train_X_lz)[test_index]\n",
    "            Y_train, Y_test = np.asarray(train_Y_lz)[train_index], np.asarray(train_Y_lz)[test_index]\n",
    "    #         print X_train[1:10]\n",
    "    #         print X_test[1:10]\n",
    "\n",
    "\n",
    "            mlp_lz = MLPRegressor(solver='lbfgs', hidden_layer_sizes=450, max_iter=150, shuffle=True, random_state=1, activation='relu')\n",
    "            mlp_lz.fit(X_train, Y_train)\n",
    "            score = mlp_lz.score(X_test, Y_test)\n",
    "#             print 'Score lz: ',score\n",
    "\n",
    "    #         lz_clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(h_size, 1), random_state=1)\n",
    "    #         lz_clf.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "\n",
    "    ft_file = open('Q4/AnsFastText.txt', 'w')\n",
    "    lz_file = open('Q4/AnsLzaridou.txt', 'w')\n",
    "    mod_ft_file = open('Q4/AnsModel.txt', 'w')\n",
    "#     mod_lz_file = open('Q4/AnsModelLZ.txt', 'w')\n",
    "\n",
    "    ft_c = 0.0\n",
    "    lz_c = 0.0\n",
    "    tot = 0\n",
    "    for i in range(len(root)):\n",
    "        ft_file.write(derived[i])\n",
    "        out_1_ft = fasttext_dict[derived[i]]\n",
    "        for k in out_1_ft:\n",
    "            ft_file.write(' '+str(k))\n",
    "        ft_file.write('\\n')\n",
    "\n",
    "        lz_file.write(derived[i])\n",
    "        out_1_lz = lazaridou_dict[derived[i]]\n",
    "        for k in out_1_lz:\n",
    "            lz_file.write(' '+str(k))\n",
    "        lz_file.write('\\n')\n",
    "\n",
    "        zer = [0]*len(name)\n",
    "        zer[aff_dict[affix[i]]] = 1\n",
    "        train_X = [[1]+ fasttext_dict[root[i]] + zer]\n",
    "\n",
    "        mod_ft_file.write(derived[i])\n",
    "\n",
    "        out_2_ft = mlp_ft.predict(train_X)\n",
    "\n",
    "        for k in out_2_ft:\n",
    "            mod_ft_file.write(' '+str(k))\n",
    "        mod_ft_file.write('\\n')\n",
    "\n",
    "        zer = [0]*len(name)\n",
    "        zer[aff_dict[affix[i]]] = 1\n",
    "        train_X = [[1]+ lazaridou_dict[root[i]] + zer]\n",
    "\n",
    "#         mod_lz_file.write(derived[i])\n",
    "\n",
    "        out_2_lz = mlp_lz.predict(train_X)\n",
    "#         for k in out_2_lz:\n",
    "#             mod_lz_file.write(' '+str(k))\n",
    "#         mod_lz_file.write('\\n')\n",
    "\n",
    "        cos_ft = np.inner(np.asarray(out_1_ft), out_2_ft)/(np.linalg.norm(np.asarray(out_1_ft)) * np.linalg.norm(out_2_ft))\n",
    "        cos_lz = np.inner(np.asarray(out_1_lz), out_2_lz)/(np.linalg.norm(np.asarray(out_1_lz)) * np.linalg.norm(out_2_lz))\n",
    "\n",
    "        ft_c += cos_ft\n",
    "        lz_c += cos_lz\n",
    "        tot += 1\n",
    "\n",
    "\n",
    "    ft_file.close()\n",
    "    lz_file.close()\n",
    "    mod_ft_file.close()\n",
    "#     mod_lz_file.close()\n",
    "\n",
    "    cosVal1 = ft_c/tot\n",
    "    cosVal2 = lz_c/tot\n",
    "\n",
    "    return cosVal1, cosVal2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85742137] [ 0.32482805]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    similarityTask()\n",
    "    anaSim = analogyTask()\n",
    "    derCos1,derCos2 = derivedWordTask()\n",
    "    print anaSim\n",
    "    print derCos1, derCos2\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# word2vec = pickle.load(open('Q1/word2vec.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_X_raw = []\n",
    "# train_Y_raw = []\n",
    "# train_X = []\n",
    "# train_Y = []\n",
    "\n",
    "# keys = trainDict.keys()\n",
    "\n",
    "\n",
    "# for i in range(20000):\n",
    "#     try:\n",
    "#         arr = np.random.randint(13, size = 2)\n",
    "#         word_1 = random.choice(trainDict[keys[arr[0]]]).split('\\t')\n",
    "#         word_2 = random.choice(trainDict[keys[arr[1]]]).split('\\t')\n",
    "#         train_X_raw += [[1]+ word2vec[word_1[0].lower()]+ word2vec[word_1[1].lower()]+ word2vec[word_2[0].lower()]+ word2vec[word_2[1].lower()]]\n",
    "#         train_Y_raw += [0]\n",
    "#     except:\n",
    "#         continue\n",
    "\n",
    "# for i in range(20000):\n",
    "#     try:\n",
    "#         arr = np.random.randint(13)\n",
    "#         word_1 = random.choice(trainDict[keys[arr]]).split('\\t')\n",
    "#         word_2 = random.choice(trainDict[keys[arr]]).split('\\t')\n",
    "#         train_X_raw += [[1] + word2vec[word_1[0].lower()] + word2vec[word_1[1].lower()] + word2vec[word_2[0].lower()] + word2vec[word_2[1].lower()]]\n",
    "#         train_Y_raw += [1]\n",
    "#     except:\n",
    "#         continue\n",
    "        \n",
    "# index_shuf = range(len(train_Y_raw))\n",
    "# random.shuffle(index_shuf)\n",
    "# for i in index_shuf:\n",
    "#     train_X.append(train_X_raw[i])\n",
    "#     train_Y.append(train_Y_raw[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print len(train_X)\n",
    "# print len(train_Y)\n",
    "# print train_Y[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Layer's sizes\n",
    "# x_size = 4 * 300 + 1   # Number of input nodes: 4 features and 1 bias\n",
    "\n",
    "# h_size = 300                # Number of hidden nodes\n",
    "# y_size = 1   # Number of outcomes (3 iris flowers)\n",
    "\n",
    "# train_size = len(train_Y)\n",
    "\n",
    "# clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(h_size, 1), random_state=1)\n",
    "\n",
    "# clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neg = 0\n",
    "# pos = 0\n",
    "# flag = 0\n",
    "# for item in analogyDataset:\n",
    "#     query = item[0].split(' ')\n",
    "#     test_X = []\n",
    "#     test_Y = []\n",
    "    \n",
    "    \n",
    "#     for num in range(1,6):\n",
    "#         option = item[num].split(' ')\n",
    "#         try:\n",
    "#             test_X += [[1] + word2vec[query[0].lower()] + word2vec[query[1].lower()] + word2vec[option[0].lower()] + word2vec[option[1].lower()]]\n",
    "#         except:\n",
    "#             test_X += [[0.0]*(4*300+1)]\n",
    "    \n",
    "#     output = np.argmax(clf.predict_proba(test_X)[1])\n",
    "#     test_Y = ord(item[-1])-97\n",
    "    \n",
    "# #     print output\n",
    "# #     print test_Y\n",
    "    \n",
    "#     if (output == test_Y):\n",
    "#             neg += 1\n",
    "#     else:\n",
    "#             pos += 1\n",
    "\n",
    "            \n",
    "# print neg\n",
    "# print pos\n",
    "# print float(pos)/(pos+neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = np.asarray([0.2, 0.5, 0.1, 0.3])\n",
    "# b = np.argsort(a)\n",
    "# print b\n",
    "# print np.sort(a)\n",
    "# print 1+np.where(b==0)[0][0]\n",
    "# print 1+np.where(b==1)[0][0]\n",
    "# print 1+np.where(b==2)[0][0]\n",
    "# print 1+np.where(b==3)[0][0]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
