{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Deep Learning Programming Assignment 2\n",
    "--------------------------------------\n",
    "Name: Vishnu Dutt Sharma\n",
    "Roll No.: 12EC35018\n",
    "\n",
    "\n",
    "======================================\n",
    "\n",
    "Problem Statement:\n",
    "This assignment has two problems.\n",
    "\n",
    "P1. Implement a simple 1 or 2 hidden layer MLP USING any deep learning library\n",
    "for predicting MNIST images.\n",
    "P2. Implement a simple CNN USING any deep learning library\n",
    "for predicting MNIST images.\n",
    "\n",
    "Resources:\n",
    "1. https://www.tensorflow.org/get_started/mnist/beginners\n",
    "2. https://www.tensorflow.org/get_started/mnist/pros\n",
    "\n",
    "======================================\n",
    "\n",
    "Instructions:\n",
    "1. Download the MNIST dataset from http://yann.lecun.com/exdb/mnist/\n",
    "    (four files).\n",
    "2. Extract all the files into a folder named `data' just outside\n",
    "    the folder containing the main.py file. This code reads the\n",
    "    data files from the folder '../data'.\n",
    "3. Complete the functions in the train_dense.py and train_cnn.py files. You might also\n",
    "    create other functions for your convenience, but do not change anything\n",
    "    in the main.py file or the function signatures of the train and test\n",
    "    functions in the train files.\n",
    "4. The train function must train the neural network given the training\n",
    "    examples and save the in a folder named `weights' in the same\n",
    "    folder as main.py\n",
    "5. The test function must read the saved weights and given the test\n",
    "    examples it must return the predicted labels.\n",
    "6. Submit your project folder with the weights. Note: Don't include the\n",
    "    data folder, which is anyway outside your project folder.\n",
    "\n",
    "Submission Instructions:\n",
    "1. Fill your name and roll no in the space provided above.\n",
    "2. Name your folder in format <Roll No>_<First Name>.\n",
    "    For example 12CS10001_Rohan\n",
    "3. Submit a zipped format of the file (.zip only).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import train_dense\n",
    "import train_cnn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "def train_nn(trainX, trainY):\n",
    "    \n",
    "    trainX_mod = trainX.reshape((trainX.shape[0], trainX.shape[1]*trainX.shape[2]))\n",
    "    y_mod = []\n",
    "    for i in trainY:\n",
    "        zer = [0]*10\n",
    "        zer[i] = 1\n",
    "        y_mod.append(zer)\n",
    "\n",
    "    trainY_mod = np.asarray(y_mod)\n",
    "    \n",
    "    n_hidden = 400\n",
    "    n_input = 784\n",
    "    n_classes = 10\n",
    "\n",
    "    n_hidden_1 = 400  # 1st layer number of features\n",
    "    n_hidden_2 = 100# 2nd layer number of features\n",
    "    n_input = trainX_mod.shape[1] # MNIST data input (img shape: 28*28)\n",
    "    n_classes = trainY_mod.shape[1] # MNIST total classes (0-9 digits)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    X = tf.placeholder(\"float\", shape=[None, n_input])\n",
    "    y = tf.placeholder(\"float\", shape=[None, n_classes])\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_input])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    predict = multilayer_perceptron(X, weights, biases)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=y))\n",
    "    updates = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    batch_size = 1000\n",
    "    iterat = 50\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        train_X, test_X, train_y, test_y = train_test_split(trainX_mod, trainY_mod, test_size=0.20, random_state=random.randint(1,99))\n",
    "        # Train with each example\n",
    "        for i in range(len(train_X)/batch_size):\n",
    "            sess.run(updates, feed_dict={X: train_X[i*batch_size: (i + 1)*batch_size], y: train_y[i*batch_size: (i + 1)*batch_size]})\n",
    "\n",
    "        train_accuracy = np.mean(np.argmax(train_y, axis=1) ==\n",
    "                                 np.argmax(sess.run(predict, feed_dict={X: train_X, y: train_y}), axis=1))\n",
    "        test_accuracy  = np.mean(np.argmax(test_y, axis=1) ==\n",
    "                                 np.argmax(sess.run(predict, feed_dict={X: test_X, y: test_y}), axis=1))\n",
    "\n",
    "        accuracy += test_accuracy\n",
    "        print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\"\n",
    "              % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    w1= sess.run(weights['h1'])\n",
    "    np.save('./output/weight_nn_1', w1)\n",
    "\n",
    "    w2= sess.run(weights['h2'])\n",
    "    np.save('./output/weight_nn_2', w2)\n",
    "    \n",
    "    wout= sess.run(weights['out'])\n",
    "    np.save('./output/weight_nn_out', wout)\n",
    "\n",
    "    b1= sess.run(biases['b1'])\n",
    "    np.save('./output/bias_nn_1', b1)\n",
    "\n",
    "    b2= sess.run(biases['b2'])\n",
    "    np.save('./output/bias_nn_2', b2)\n",
    "    \n",
    "    bout= sess.run(biases['out'])\n",
    "    np.save('./output/bias_nn_out', bout)\n",
    "\n",
    "    \n",
    "#     w1= sess.run(weights['h1'])\n",
    "#     np.save('weight1', w1)\n",
    "    \n",
    "#     w2= sess.run(weights['out'])\n",
    "#     np.save('weight2', w2)\n",
    "    \n",
    "#     b1= sess.run(biases['b1'])\n",
    "#     np.save('bias1', b1)\n",
    "    \n",
    "#     b2= sess.run(biases['out'])\n",
    "#     np.save('bias2', b2)\n",
    "    \n",
    "    \n",
    "    sess.close()\n",
    "    '''\n",
    "    Complete this function.\n",
    "    '''\n",
    "\n",
    "def test_nn(testX):\n",
    "    testX_mod = testX.reshape((testX.shape[0], testX.shape[1]*testX.shape[2]))\n",
    "    \n",
    "    n_hidden = 400\n",
    "    n_input = 784\n",
    "    n_classes = 10\n",
    "\n",
    "    n_hidden_1 = 500  # 1st layer number of features\n",
    "    n_hidden_2 = 150# 2nd layer number of features\n",
    "    n_input = testX_mod.shape[1] # MNIST data input (img shape: 28*28)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    X = tf.placeholder(\"float\", shape=[None, n_input])\n",
    "    \n",
    "    weights = dict()\n",
    "    biases = dict()\n",
    "    \n",
    "    weights['h1'] = np.load('./output/weight_nn_1.npy')\n",
    "    weights['h2'] = np.load('./output/weight_nn_2.npy')\n",
    "    weights['out'] = np.load('./output/weight_nn_out.npy')\n",
    "\n",
    "    biases['b1'] = np.load('./output/bias_nn_1.npy')\n",
    "    biases['b2'] = np.load('./output/bias_nn_2.npy')\n",
    "    biases['out'] = np.load('./output/bias_nn_out.npy')\n",
    "    \n",
    "  \n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_input])\n",
    "    \n",
    "    predict = multilayer_perceptron(X, weights, biases)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    output = np.argmax(sess.run(predict, feed_dict={X: testX_mod}), axis=1)\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "#function to convert matrix to vector\n",
    "def cnn_convertToVector(matrix):   \n",
    "    vector = matrix.reshape((matrix.shape[0], matrix.shape[1]*matrix.shape[2])).astype(np.float)\n",
    "    vector = vector - vector.mean(axis=0)\n",
    "    return vector\n",
    "\n",
    "def cnn_InitWeights(shape):\n",
    "    weight = tf.random_normal(shape, stddev = 0.1)\n",
    "    weight = tf.cast(weight, tf.float32) \n",
    "    return tf.Variable(weight)\n",
    "\n",
    "def cnn_InitBias(shape):\n",
    "    bias = tf.constant(1, shape = shape)\n",
    "    bias = tf.cast(bias, tf.float32) \n",
    "    return tf.Variable(bias)\n",
    "\n",
    "def cnn_ForwardPropogation(X, w_1, b_1, w_2, b_2, w_fc1, b_fc1, w_fc2, b_fc2):\n",
    "    X = tf.reshape(X, [-1,28,28,1])\n",
    "    h_conv1 = tf.nn.sigmoid(tf.nn.conv2d(X, w_1, strides = [1,1,1,1], padding = 'SAME') + b_1)\n",
    "    h_pool1 = tf.nn.max_pool(h_conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    \n",
    "    h_conv2 = tf.nn.sigmoid(tf.nn.conv2d(h_pool1, w_2, strides = [1,1,1,1], padding = 'SAME') + b_2)\n",
    "    h_pool2 = tf.nn.max_pool(h_conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME', )\n",
    "    \n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "\n",
    "    h_fc1 = tf.nn.sigmoid(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "    y_conv = tf.matmul(h_fc1, w_fc2) + b_fc2\n",
    "    return y_conv\n",
    "\n",
    "def cnn_createData(X, Y):\n",
    "    X = cnn_convertToVector(X)\n",
    "    num_labels = len(np.unique(Y))\n",
    "    Y_temp = np.eye(num_labels)[Y] \n",
    "    return X, Y_temp\n",
    "    \n",
    "def train_cnn_train(trainX, trainY):\n",
    "    \n",
    "    # Symbols\n",
    "    X_all,Y_all  = cnn_createData(trainX, trainY)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X_all, Y_all, test_size=0.10, random_state=RANDOM_SEED)    \n",
    "    \n",
    "    print(\"Train Size: \")\n",
    "    print(train_X.shape)\n",
    "    print(\"Test Size: \")\n",
    "    print(test_X.shape)\n",
    "    \n",
    "    x_size = train_X.shape[1] \n",
    "    y_size = train_y.shape[1]\n",
    "    X = tf.placeholder(\"float32\", shape=[None, x_size])\n",
    "    y = tf.placeholder(\"float32\", shape=[None, y_size])\n",
    "    \n",
    "    # Weight & bias initializations\n",
    "    w_1 = cnn_InitWeights([4,4,1,32])\n",
    "    b_1 = cnn_InitBias([32])\n",
    "    w_2 = cnn_InitWeights([4,4,32,64])\n",
    "    b_2 = cnn_InitBias([64])\n",
    "    w_fc1 = cnn_InitWeights([64*7*7, 1024])\n",
    "    b_fc1 = cnn_InitBias([1024])\n",
    "    w_fc2 = cnn_InitWeights([1024, 10])\n",
    "    b_fc2 = cnn_InitBias([10])\n",
    "     \n",
    "    # Forward propagation\n",
    "    yhat = cnn_ForwardPropogation(X, w_1, b_1, w_2, b_2, w_fc1, b_fc1, w_fc2, b_fc2)\n",
    "    predict = tf.argmax(yhat, axis=1)\n",
    "    \n",
    "    # Back propogation\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = yhat))\n",
    "    updates = tf.train.AdamOptimizer(1e-3).minimize(cost)\n",
    "    \n",
    "     # Run SGD    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    num_epoch = 1\n",
    "    data_length = train_X.shape[0] \n",
    "    batch_size = 5000\n",
    "    number_batches = int(np.ceil(data_length/batch_size))    \n",
    "        \n",
    "    for epoch in range(num_epoch):\n",
    "        batches = list(range(number_batches))\n",
    "        random.shuffle(batches)\n",
    "        for batch_index in batches:\n",
    "            if((batch_index+1)*batch_size < data_length):\n",
    "                batch_X = train_X[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "                batch_Y = train_y[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "            else:\n",
    "                batch_X = train_X[batch_index*batch_size:data_length]\n",
    "                batch_Y = train_y[batch_index*batch_size:data_length]\n",
    "\n",
    "            # Train with each example\n",
    "            train_accuracy = 0\n",
    "            for i in range(len(batch_X)):\n",
    "                sess.run(updates, feed_dict={X: batch_X[i: i + 1], y: batch_Y[i: i + 1]})\n",
    "                if(np.argmax(batch_Y[i: i + 1], axis=1) == sess.run(predict, feed_dict={X: batch_X[i: i + 1], y: batch_Y[i: i + 1]})):\n",
    "                    train_accuracy += 1       \n",
    "            train_accuracy /= len(batch_X)\n",
    "            \n",
    "            test_accuracy = 0\n",
    "            for i in range(len(test_y)):\n",
    "                if(np.argmax(test_y[i: i + 1], axis=1) == sess.run(predict, feed_dict={X: test_X[i: i + 1], y: test_y[i: i + 1]})):\n",
    "                    test_accuracy += 1\n",
    "            test_accuracy /= len(test_y)\n",
    "            \n",
    "#             train_accuracy = np.mean(np.argmax(batch_Y, axis=1) == sess.run(predict, feed_dict={X: batch_X[i: i + 1], y: batch_Y[i: i + 1]}))\n",
    "#             test_accuracy = 0\n",
    "#             test_accuracy  = np.mean(np.argmax(test_y, axis=1) == sess.run(predict, feed_dict={X: test_X, y: test_y}))\n",
    "            print(\"CNN: Epoch = %d/%d,  Batch = %d/%d, Train Accuracy = %.4f, Test Accuracy = %.4f\" \n",
    "                  % (epoch + 1,num_epoch, batch_index + 1, number_batches,100*train_accuracy, 100*test_accuracy))\n",
    "    \n",
    "    fw_1 = sess.run(w_1)\n",
    "    fw_2 = sess.run(w_2)\n",
    "    fw_fc1 = sess.run(w_fc1)\n",
    "    fw_fc2 = sess.run(w_fc2)\n",
    "    \n",
    "    fb_1 = sess.run(b_1)\n",
    "    fb_2 = sess.run(b_2)\n",
    "    fb_fc1 = sess.run(b_fc1)\n",
    "    fb_fc2 = sess.run(b_fc2)\n",
    "    \n",
    "    \n",
    "    np.save('./output/weight_cnn' ,[fw_1, fw_2, fw_fc1, fw_fc2, fb_1, fb_2, fb_fc1, fb_fc2])\n",
    "    sess.close()\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"CNN: Weigths and Biases saved !!\")\n",
    "    return    \n",
    "\n",
    "def train_cnn_test(testX):\n",
    "    \n",
    "    testX = cnn_convertToVector(testX) \n",
    "    x_size = testX.shape[1] \n",
    "    \n",
    "    [w_1, w_2, w_fc1, w_fc2, b_1, b_2, b_fc1, b_fc2] = np.load('./output/cnn_weight')\n",
    "    \n",
    "    print(\"CNN: Weigths and Biases restored !!\")\n",
    "    \n",
    "     # Symbols  \n",
    "    X = tf.placeholder(\"float32\", shape=[None, x_size])\n",
    "    \n",
    "    yhat = cnn_ForwardPropogation(X, w_1, b_1, w_2, b_2, w_fc1, b_fc1, w_fc2, b_fc2)\n",
    "    predict = tf.argmax(yhat, axis=1)\n",
    "        \n",
    "    testY = np.zeros(testX.shape[0], dtype = 'int')\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        for i in range(testX.shape[0]):\n",
    "            testY[i: i+1] = sess.run(predict, feed_dict={X: testX[i: i + 1]})\n",
    "        \n",
    "\n",
    "#     with tf.Session() as sess:        \n",
    "#         testY = sess.run(predict, feed_dict={X: testX})  \n",
    "    return testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    data_dir = '../data'\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 'train-images-idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    trX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 'train-labels-idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    trY = loaded[8:].reshape((60000)).astype(np.int)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 't10k-images-idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    teX = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 't10k-labels-idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    teY = loaded[8:].reshape((10000)).astype(np.int)\n",
    "\n",
    "    trY = np.asarray(trY)\n",
    "    teY = np.asarray(teY)\n",
    "\n",
    "    perm = np.random.permutation(trY.shape[0])\n",
    "    trX = trX[perm]\n",
    "    trY = trY[perm]\n",
    "\n",
    "    perm = np.random.permutation(teY.shape[0])\n",
    "    teX = teX[perm]\n",
    "    teY = teY[perm]\n",
    "\n",
    "    return trX, trY, teX, teY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_digit(digit_pixels, label='?'):\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            if digit_pixels[i, j] > 128:\n",
    "                print '#',\n",
    "            else:\n",
    "                print '.',\n",
    "        print ''\n",
    "\n",
    "    print 'Label: ', label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# def conv2d_1(x, W):\n",
    "#     return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# def conv2d(x, W):\n",
    "#     return tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_3x3(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def train_cnn(trainX, trainY):\n",
    "    \n",
    "    '''\n",
    "    Complete this function.\n",
    "    '''\n",
    "    \n",
    "    # Network Parameters\n",
    "    n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "    n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "    dropout = 0.5 # Dropout, probability to keep units\n",
    "    learning_rate = 0.001\n",
    "    training_iters = 200000\n",
    "    batch_size = 128\n",
    "    display_step = 10\n",
    "\n",
    "\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        # 5x5 conv, 1 input, 32 outputs\n",
    "        'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32], 0.1)),\n",
    "        # 5x5 conv, 32 inputs, 64 outputs\n",
    "        'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64], 0.1)),\n",
    "        # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "        'wd1': tf.Variable(tf.random_normal([7*7*64, 1024], 0.1)),\n",
    "        # 1024 inputs, 10 outputs (class prediction)\n",
    "        'out': tf.Variable(tf.random_normal([1024, n_classes], 0.1))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.random_normal([32], 0.1)),\n",
    "        'bc2': tf.Variable(tf.random_normal([64], 0.1)),\n",
    "        'bd1': tf.Variable(tf.random_normal([1024], 0.1)),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], 0.1))\n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    pred = conv_net(x, weights, biases, keep_prob)\n",
    "#     y_conv = tf.nn.softmax(pred)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "#     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    cost  = -tf.reduce_sum(y * tf.log(tf.clip_by_value(y_conv, 1e-10,1.0)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    y_mod = []\n",
    "    for i in trainY:\n",
    "        zer = [0]*10\n",
    "        zer[i] = 1\n",
    "        y_mod.append(zer)\n",
    "\n",
    "    trainY_mod = np.asarray(y_mod)\n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    # Launch the graph\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = trainX[step*batch_size : (step+1)*batch_size], trainY_mod[step*batch_size : (step+1)*batch_size]\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    \n",
    "    \n",
    "#     saver = tf.train.Saver({\"wc1\": weights['wc1'], \"wc2\": weights['wc2'], \"wd1\": weights['wd1'], \"wout\": weights['out'], \"bc1\": biases['bc1'], \"bc2\": biases['bc2'], \"bd1\": biases['bd1'],  \"bout\": biases['out'],})\n",
    "#     saver.save(sess, 'output/model.ckpt')\n",
    "    \n",
    "    var = sess.run( weights['wc1'] )\n",
    "    np.save('wc1', var)\n",
    "    \n",
    "    var = sess.run( weights['wc2'] )\n",
    "    np.save('wc2', var)\n",
    "    \n",
    "    var = sess.run( weights['wd1'] )\n",
    "    np.save('wd1', var)\n",
    "    \n",
    "    var = sess.run( weights['out'] )\n",
    "    np.save('wout', var)\n",
    "    \n",
    "    var = sess.run( biases['bc1'] )\n",
    "    np.save('bc1', var)\n",
    "    \n",
    "    var = sess.run( biases['bc2'] )\n",
    "    np.save('bc2', var)\n",
    "    \n",
    "    var = sess.run( biases['bd1'] )\n",
    "    np.save('bd1', var)\n",
    "    \n",
    "    var = sess.run( biases['out'] )\n",
    "    np.save('bout', var)\n",
    "    \n",
    "    \n",
    "#     var = weights['wc1']\n",
    "#     saver = tf.train.Saver({\"wc1\": var})\n",
    "#     saver.save(sess, 'w_c1')\n",
    "    \n",
    "#     var = weights['wc2']\n",
    "#     saver = tf.train.Saver({\"wc2\": var})\n",
    "#     saver.save(sess, 'w_c2')\n",
    "    \n",
    "#     var = weights['wd1']\n",
    "#     saver = tf.train.Saver({\"wd1\": var})\n",
    "#     saver.save(sess, 'w_d1')\n",
    "    \n",
    "#     var = weights['out']\n",
    "#     saver = tf.train.Saver({\"w_out\": var})\n",
    "#     saver.save(sess,'w_out')\n",
    "    \n",
    "#     var = biases['bc1']\n",
    "#     saver = tf.train.Saver({\"bc1\": var})\n",
    "#     saver.save(sess, 'b_c1')\n",
    "    \n",
    "#     var = biases['bc2']\n",
    "#     saver = tf.train.Saver({\"bc2\": var})\n",
    "#     saver.save(sess, 'b_c2')\n",
    "    \n",
    "#     var = biases['bd1']\n",
    "#     saver = tf.train.Saver({\"bd1\": var})\n",
    "#     saver.save(sess, 'b_d1')\n",
    "    \n",
    "#     var = biases['out']\n",
    "#     saver = tf.train.Saver({\"b_out\": var})\n",
    "#     saver.save(sess, 'b_out')\n",
    "    \n",
    "   \n",
    "    sess.close()\n",
    "    \n",
    "    \n",
    "#     n_hidden = 400\n",
    "#     n_input = 784\n",
    "#     n_classes = 10\n",
    "\n",
    "    \n",
    "#     x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "#     y_ = tf.placeholder(tf.int64, [None, 10])\n",
    "#     y = tf.placeholder(tf.int64, [None, ])\n",
    "\n",
    "   \n",
    "    \n",
    "#     W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "#     b_conv1 = bias_variable([32])\n",
    "\n",
    "#     h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "#     h_pool1 = max_pool_3x3(h_conv1)\n",
    "\n",
    "#     W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "#     b_conv2 = bias_variable([64])\n",
    "\n",
    "#     h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#     h_pool2 = max_pool_3x3(h_conv2)\n",
    "    \n",
    "#     W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "#     b_fc1 = bias_variable([1024])\n",
    "\n",
    "#     h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "#     h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#     keep_prob = tf.placeholder(tf.float32)\n",
    "#     h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#     W_fc2 = weight_variable([1024, 10])\n",
    "#     b_fc2 = bias_variable([10])\n",
    "\n",
    "#     y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    \n",
    "#     cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    \n",
    "#     train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    \n",
    "#     correct_prediction = tf.equal(tf.argmax(y_conv,1), y_)\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "#     y_mod = []\n",
    "#     for i in trainY:\n",
    "#         zer = [0]*10\n",
    "#         zer[i] = 1\n",
    "#         y_mod.append(zer)\n",
    "\n",
    "#     trainY_mod = np.asarray(y_mod)\n",
    "    \n",
    "    \n",
    "#     sess = tf.InteractiveSession()\n",
    "    \n",
    "#     tf.global_variables_initializer().run()\n",
    "    \n",
    "#     epoch = 0\n",
    "#     batch_size = 1000\n",
    "    \n",
    "#     print x.shape\n",
    "#     print trainX.shape\n",
    "#     print y_.shape\n",
    "#     print trainY.shape\n",
    "#     print trainY_mod.shape\n",
    "    \n",
    "#     for i in range(10):\n",
    "    \n",
    "#         for i in range(batch_size):\n",
    "#             batch_xs, batch_ys = trainX[i*batch_size : (i+1)*batch_size], trainY_mod[i*batch_size : (i+1)*batch_size]\n",
    "#             train_step.run(feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "\n",
    "# #             sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "    \n",
    "#         train_accuracy = accuracy.eval(feed_dict={x: trainX, y: trainY, keep_prob: 0.5})\n",
    "\n",
    "#         print 'Epoch # ', epoch+1, 'Accuracy: ', train_accuracy*100\n",
    "#         epoch += 1\n",
    "\n",
    "    \n",
    "#     np.save('W_conv1', W_conv1)\n",
    "#     np.save('W_conv2', W_conv2)\n",
    "#     np.save('W_fc1', W_fc1)\n",
    "#     np.save('W_fc2', W_fc2)\n",
    "    \n",
    "#     np.save('b_conv1', b_conv1)\n",
    "#     np.save('b_conv2', b_conv2)\n",
    "#     np.save('b_fc1', b_fc1)\n",
    "#     np.save('b_fc2', b_fc2)\n",
    "\n",
    "#     sess.close()\n",
    "\n",
    "\n",
    "\n",
    "def test_cnn(testX):\n",
    "    '''\n",
    "    Complete this function.\n",
    "    This function must read the weight files and\n",
    "    return the predicted labels.\n",
    "    The returned object must be a 1-dimensional numpy array of\n",
    "    length equal to the number of examples. The i-th element\n",
    "    of the array should contain the label of the i-th test\n",
    "    example.\n",
    "    '''\n",
    "    # Network Parameters\n",
    "    n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "    n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "    dropout = 0.5 # Dropout, probability to keep units\n",
    "    learning_rate = 0.001\n",
    "    training_iters = 200000\n",
    "    batch_size = 128\n",
    "    display_step = 10\n",
    "    \n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "    \n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        # 5x5 conv, 1 input, 32 outputs\n",
    "        'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32], 0.1)),\n",
    "        # 5x5 conv, 32 inputs, 64 outputs\n",
    "        'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64], 0.1)),\n",
    "        # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "        'wd1': tf.Variable(tf.random_normal([7*7*64, 1024], 0.1)),\n",
    "        # 1024 inputs, 10 outputs (class prediction)\n",
    "        'out': tf.Variable(tf.random_normal([1024, n_classes], 0.1))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.random_normal([32], 0.1)),\n",
    "        'bc2': tf.Variable(tf.random_normal([64], 0.1)),\n",
    "        'bd1': tf.Variable(tf.random_normal([1024], 0.1)),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], 0.1))\n",
    "    }\n",
    "    \n",
    "    \n",
    "    weights['wc1'] = tf.Variable(np.load('wc1.npy'))\n",
    "    \n",
    "    weights['wc2'] = tf.Variable(np.load('wc2.npy'))\n",
    "    \n",
    "    weights['wd1'] = tf.Variable(np.load('wd1.npy'))\n",
    "    \n",
    "    weights['out'] = tf.Variable(np.load('wout.npy'))\n",
    "    \n",
    "    biases['bc1'] = tf.Variable(np.load('bc1.npy'))\n",
    "    \n",
    "    biases['bc2'] = tf.Variable(np.load('bc2.npy'))\n",
    "    \n",
    "    biases['bd1'] = tf.Variable(np.load('bd1.npy'))\n",
    "    \n",
    "    biases['out'] = tf.Variable(np.load('bout.npy'))\n",
    "    \n",
    "        \n",
    "#     Construct model\n",
    "    pred = conv_net(x, weights, biases, keep_prob)\n",
    "    \n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    \n",
    "  \n",
    "    \n",
    "    out = sess.run(pred, feed_dict={x: testX, keep_prob: dropout})\n",
    "    sess.close()\n",
    "    \n",
    "    return np.argmax(out, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trainX, trainY, testX, testY = load_mnist()\n",
    "def main():\n",
    "    trainX, trainY, testX, testY = load_mnist()\n",
    "    print \"Shapes: \", trainX.shape, trainY.shape, testX.shape, testY.shape\n",
    "\n",
    "#     print \"\\nDigit sample\"\n",
    "#     print_digit(trainX[1], trainY[1])\n",
    "    \n",
    "#     train_nn(trainX, trainY)\n",
    "#     labels = test_nn(testX)\n",
    "#     accuracy = np.mean((labels == testY)) * 100.0\n",
    "#     print \"\\nDNN Test accuracy: %lf%%\" % accuracy\n",
    "\n",
    "    train_cnn_train(trainX, trainY)\n",
    "    labels = train_cnn_test(testX)\n",
    "    accuracy = np.mean((labels == testY)) * 100.0\n",
    "    print \"\\nCNN Test accuracy: %lf%%\" % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  (60000, 28, 28, 1) (60000,) (10000, 28, 28, 1) (10000,)\n",
      "Train Size: \n",
      "(54000, 784)\n",
      "Test Size: \n",
      "(6000, 784)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.eye(10)[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
