{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDeep Learning Programming Assignment 2\\n--------------------------------------\\nName: Vishnu Dutt Sharma\\nRoll No.: 12EC35018\\n\\n\\n======================================\\n\\nProblem Statement:\\nThis assignment has two problems.\\n\\nP1. Implement a simple 1 or 2 hidden layer MLP USING any deep learning library\\nfor predicting MNIST images.\\nP2. Implement a simple CNN USING any deep learning library\\nfor predicting MNIST images.\\n\\nResources:\\n1. https://www.tensorflow.org/get_started/mnist/beginners\\n2. https://www.tensorflow.org/get_started/mnist/pros\\n\\n======================================\\n\\nInstructions:\\n1. Download the MNIST dataset from http://yann.lecun.com/exdb/mnist/\\n    (four files).\\n2. Extract all the files into a folder named `data' just outside\\n    the folder containing the main.py file. This code reads the\\n    data files from the folder '../data'.\\n3. Complete the functions in the train_dense.py and train_cnn.py files. You might also\\n    create other functions for your convenience, but do not change anything\\n    in the main.py file or the function signatures of the train and test\\n    functions in the train files.\\n4. The train function must train the neural network given the training\\n    examples and save the in a folder named `weights' in the same\\n    folder as main.py\\n5. The test function must read the saved weights and given the test\\n    examples it must return the predicted labels.\\n6. Submit your project folder with the weights. Note: Don't include the\\n    data folder, which is anyway outside your project folder.\\n\\nSubmission Instructions:\\n1. Fill your name and roll no in the space provided above.\\n2. Name your folder in format <Roll No>_<First Name>.\\n    For example 12CS10001_Rohan\\n3. Submit a zipped format of the file (.zip only).\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Deep Learning Programming Assignment 2\n",
    "--------------------------------------\n",
    "Name: Vishnu Dutt Sharma\n",
    "Roll No.: 12EC35018\n",
    "\n",
    "\n",
    "======================================\n",
    "\n",
    "Problem Statement:\n",
    "This assignment has two problems.\n",
    "\n",
    "P1. Implement a simple 1 or 2 hidden layer MLP USING any deep learning library\n",
    "for predicting MNIST images.\n",
    "P2. Implement a simple CNN USING any deep learning library\n",
    "for predicting MNIST images.\n",
    "\n",
    "Resources:\n",
    "1. https://www.tensorflow.org/get_started/mnist/beginners\n",
    "2. https://www.tensorflow.org/get_started/mnist/pros\n",
    "\n",
    "======================================\n",
    "\n",
    "Instructions:\n",
    "1. Download the MNIST dataset from http://yann.lecun.com/exdb/mnist/\n",
    "    (four files).\n",
    "2. Extract all the files into a folder named `data' just outside\n",
    "    the folder containing the main.py file. This code reads the\n",
    "    data files from the folder '../data'.\n",
    "3. Complete the functions in the train_dense.py and train_cnn.py files. You might also\n",
    "    create other functions for your convenience, but do not change anything\n",
    "    in the main.py file or the function signatures of the train and test\n",
    "    functions in the train files.\n",
    "4. The train function must train the neural network given the training\n",
    "    examples and save the in a folder named `weights' in the same\n",
    "    folder as main.py\n",
    "5. The test function must read the saved weights and given the test\n",
    "    examples it must return the predicted labels.\n",
    "6. Submit your project folder with the weights. Note: Don't include the\n",
    "    data folder, which is anyway outside your project folder.\n",
    "\n",
    "Submission Instructions:\n",
    "1. Fill your name and roll no in the space provided above.\n",
    "2. Name your folder in format <Roll No>_<First Name>.\n",
    "    For example 12CS10001_Rohan\n",
    "3. Submit a zipped format of the file (.zip only).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# import train_dense\n",
    "# import train_cnn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "def train_nn(trainX, trainY):\n",
    "    \n",
    "    trainX_mod = trainX.reshape((trainX.shape[0], trainX.shape[1]*trainX.shape[2]))\n",
    "#     trainX_mod = trainX_mod - trainX_mod.mean(axis=0)\n",
    "    y_mod = []\n",
    "    for i in trainY:\n",
    "        zer = [0]*10\n",
    "        zer[i] = 1\n",
    "        y_mod.append(zer)\n",
    "\n",
    "    trainY_mod = np.asarray(y_mod)\n",
    "    \n",
    "    n_hidden = 400\n",
    "    n_input = 784\n",
    "    n_classes = 10\n",
    "\n",
    "    n_hidden_1 = 400  # 1st layer number of features\n",
    "    n_hidden_2 = 100# 2nd layer number of features\n",
    "    n_input = trainX_mod.shape[1] # MNIST data input (img shape: 28*28)\n",
    "    n_classes = trainY_mod.shape[1] # MNIST total classes (0-9 digits)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    X = tf.placeholder(\"float\", shape=[None, n_input])\n",
    "    y = tf.placeholder(\"float\", shape=[None, n_classes])\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], 0.001)),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], 0.001)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes], 0.001))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1], 0.001)),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2], 0.001)),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes], 0.001))\n",
    "    }\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_input])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    predict = multilayer_perceptron(X, weights, biases)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=y))\n",
    "    updates = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    batch_size = 100\n",
    "    iterat = 20\n",
    "\n",
    "    \n",
    "    for epoch in range(iterat):\n",
    "        train_X, test_X, train_y, test_y = train_test_split(trainX_mod, trainY_mod, test_size=0.20, random_state=random.randint(1,99))\n",
    "        # Train with each example\n",
    "        for i in range(int(len(train_X)/batch_size)):\n",
    "            sess.run(updates, feed_dict={X: train_X[i*batch_size: (i + 1)*batch_size], y: train_y[i*batch_size: (i + 1)*batch_size]})\n",
    "\n",
    "        train_accuracy = np.mean(np.argmax(train_y, axis=1) ==\n",
    "                                 np.argmax(sess.run(predict, feed_dict={X: train_X, y: train_y}), axis=1))\n",
    "        test_accuracy  = np.mean(np.argmax(test_y, axis=1) ==\n",
    "                                 np.argmax(sess.run(predict, feed_dict={X: test_X, y: test_y}), axis=1))\n",
    "\n",
    "        accuracy += test_accuracy\n",
    "        print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\"\n",
    "              % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not os.path.exists('./weights/'):\n",
    "        os.makedirs('./weights/')\n",
    "    \n",
    "    w1= sess.run(weights['h1'])\n",
    "    np.save('./weights/weight_nn_1', w1)\n",
    "\n",
    "    w2= sess.run(weights['h2'])\n",
    "    np.save('./weights/weight_nn_2', w2)\n",
    "    \n",
    "    wout= sess.run(weights['out'])\n",
    "    np.save('./weights/weight_nn_out', wout)\n",
    "\n",
    "    b1= sess.run(biases['b1'])\n",
    "    np.save('./weights/bias_nn_1', b1)\n",
    "\n",
    "    b2= sess.run(biases['b2'])\n",
    "    np.save('./weights/bias_nn_2', b2)\n",
    "    \n",
    "    bout= sess.run(biases['out'])\n",
    "    np.save('./weights/bias_nn_out', bout)\n",
    "    \n",
    "    sess.close()\n",
    "\n",
    "    \n",
    "def test_nn(testX):\n",
    "    testX_mod = testX.reshape((testX.shape[0], testX.shape[1]*testX.shape[2]))\n",
    "#     testX_mod = testX_mod - testX_mod.mean(axis=0)\n",
    "    \n",
    "    n_input = 784\n",
    "    X = tf.placeholder(\"float\", shape=[None, n_input])\n",
    "    \n",
    "    weights = dict()\n",
    "    biases = dict()\n",
    "    \n",
    "    download = False\n",
    "    if download:\n",
    "        downloadData_CNN()\n",
    "    \n",
    "\n",
    "    weights['h1'] = np.load('./weights/weight_nn_1.npy')\n",
    "    weights['h2'] = np.load('./weights/weight_nn_2.npy')\n",
    "    weights['out'] = np.load('./weights/weight_nn_out.npy')\n",
    "\n",
    "    biases['b1'] = np.load('./weights/bias_nn_1.npy')\n",
    "    biases['b2'] = np.load('./weights/bias_nn_2.npy')\n",
    "    biases['out'] = np.load('./weights/bias_nn_out.npy')\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, n_input])\n",
    "    \n",
    "    predict = multilayer_perceptron(X, weights, biases)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    output = np.argmax(sess.run(predict, feed_dict={X: testX_mod}), axis=1)\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    data_dir = '../data'\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 'train-images-idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    trX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 'train-labels-idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    trY = loaded[8:].reshape((60000)).astype(np.int)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 't10k-images-idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    teX = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 't10k-labels-idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    teY = loaded[8:].reshape((10000)).astype(np.int)\n",
    "\n",
    "    trY = np.asarray(trY)\n",
    "    teY = np.asarray(teY)\n",
    "\n",
    "    perm = np.random.permutation(trY.shape[0])\n",
    "    trX = trX[perm]\n",
    "    trY = trY[perm]\n",
    "\n",
    "    perm = np.random.permutation(teY.shape[0])\n",
    "    teX = teX[perm]\n",
    "    teY = teY[perm]\n",
    "\n",
    "    return trX, trY, teX, teY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_digit(digit_pixels, label='?'):\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            if digit_pixels[i, j] > 128:\n",
    "                print('#',end='')\n",
    "            else:\n",
    "                print('.',end='')\n",
    "        print('')\n",
    "\n",
    "    print('Label: ', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadFromURL(outfile):\n",
    "    http_proxy  = \"http://10.3.100.207:8080\"\n",
    "    https_proxy = \"https://10.3.100.207:8080\"\n",
    "    ftp_proxy   = \"ftp://10.3.100.207:8080\"\n",
    "\n",
    "    proxyDict = { \n",
    "                  \"http\"  : http_proxy, \n",
    "                  \"https\" : https_proxy, \n",
    "                  \"ftp\"   : ftp_proxy\n",
    "                }\n",
    "\n",
    "    base = 'https://raw.githubusercontent.com/VishnuDuttSharma/DL_weights/master/asg3/'\n",
    "    url  = base + outfile\n",
    "    stream = requests.get(url, proxyDict)\n",
    "    data = np.load(io.BytesIO(stream.content))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def downloadData():\n",
    "    http_proxy  = \"http://10.3.100.207:8080\"\n",
    "    https_proxy = \"https://10.3.100.207:8080\"\n",
    "    ftp_proxy   = \"ftp://10.3.100.207:8080\"\n",
    "\n",
    "    proxyDict = { \n",
    "                  \"http\"  : http_proxy, \n",
    "                  \"https\" : https_proxy, \n",
    "                  \"ftp\"   : ftp_proxy\n",
    "                }\n",
    "    \n",
    "    base = 'https://raw.githubusercontent.com/VishnuDuttSharma/DL_weights/master/asg3/'\n",
    "    \n",
    "    weightList = ['bias_nn_1.npy', 'bias_nn_2.npy', 'bias_nn_out.npy', 'b_conv1.npy', 'b_conv2.npy', 'b_fc1.npy', 'b_fc2.npy', 'weight_nn_1.npy', 'weight_nn_2.npy', 'weight_nn_out.npy', 'W_conv1.npy', 'W_conv2.npy', 'W_fc1.npy', 'W_fc2.npy']\n",
    "    \n",
    "    if not os.path.exists('./weights/'):\n",
    "        os.makedirs('./weights/')\n",
    "    \n",
    "    count = 0\n",
    "    total = len(weightList)\n",
    "    \n",
    "    print('Downloading Weights')\n",
    "    for name in weightList:\n",
    "        url = base + name\n",
    "        stream = requests.get(url, proxyDict)\n",
    "        np_f = open('./weights/'+name, 'wb')\n",
    "        np_f.write(stream.content)\n",
    "        np_f.close()\n",
    "        print(count+1,'/',total,' Complete')\n",
    "        count += 1\n",
    "        \n",
    "    print('Download Complete')\n",
    "\n",
    "def downloadData_DNN():\n",
    "    http_proxy  = \"http://10.3.100.207:8080\"\n",
    "    https_proxy = \"https://10.3.100.207:8080\"\n",
    "    ftp_proxy   = \"ftp://10.3.100.207:8080\"\n",
    "\n",
    "    proxyDict = { \n",
    "                  \"http\"  : http_proxy, \n",
    "                  \"https\" : https_proxy, \n",
    "                  \"ftp\"   : ftp_proxy\n",
    "                }\n",
    "    \n",
    "    base = 'https://raw.githubusercontent.com/VishnuDuttSharma/DL_weights/master/asg3/'\n",
    "    \n",
    "    weightList = ['bias_nn_1.npy', 'bias_nn_2.npy', 'bias_nn_out.npy', 'weight_nn_1.npy', 'weight_nn_2.npy', 'weight_nn_out.npy']\n",
    "    \n",
    "    if not os.path.exists('./weights/'):\n",
    "        os.makedirs('./weights/')\n",
    "    \n",
    "    count = 0\n",
    "    total = len(weightList)\n",
    "    \n",
    "    print('Downloading DNN Weights')\n",
    "    for name in weightList:\n",
    "        url = base + name\n",
    "        stream = requests.get(url, proxyDict)\n",
    "        np_f = open('./weights/'+name, 'wb')\n",
    "        np_f.write(stream.content)\n",
    "        np_f.close()\n",
    "        print(count+1,'/',total,' Complete')\n",
    "        count += 1\n",
    "        \n",
    "    print('Download Complete')\n",
    "\n",
    "def downloadData_CNN():\n",
    "    http_proxy  = \"http://10.3.100.207:8080\"\n",
    "    https_proxy = \"https://10.3.100.207:8080\"\n",
    "    ftp_proxy   = \"ftp://10.3.100.207:8080\"\n",
    "\n",
    "    proxyDict = { \n",
    "                  \"http\"  : http_proxy, \n",
    "                  \"https\" : https_proxy, \n",
    "                  \"ftp\"   : ftp_proxy\n",
    "                }\n",
    "\n",
    "    base = 'https://raw.githubusercontent.com/VishnuDuttSharma/DL_weights/master/asg3/'\n",
    "    \n",
    "    weightList = ['b_conv1.npy', 'b_conv2.npy', 'b_fc1.npy', 'b_fc2.npy',  'W_conv1.npy', 'W_conv2.npy', 'W_fc1.npy', 'W_fc2.npy']\n",
    "    \n",
    "    if not os.path.exists('./weights/'):\n",
    "        os.makedirs('./weights/')\n",
    "    \n",
    "    count = 0\n",
    "    total = len(weightList)\n",
    "    \n",
    "    print('Downloading CNN Weights')\n",
    "    for name in weightList:\n",
    "        url = base + name\n",
    "        stream = requests.get(url, proxyDict)\n",
    "        np_f = open('./weights/'+name, 'wb')\n",
    "        np_f.write(stream.content)\n",
    "        np_f.close()\n",
    "        print(count+1,'/',total,' Complete')\n",
    "        count += 1\n",
    "        \n",
    "    print('Download Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def train_cnn_web(trainX, trainY):\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None,28,28,1])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    \n",
    "    y_mod = []\n",
    "    for i in trainY:\n",
    "        zer = [0]*10\n",
    "        zer[i] = 1\n",
    "        y_mod.append(zer)\n",
    "\n",
    "    train_Y_mod = np.asarray(y_mod)\n",
    "    \n",
    "    \n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    \n",
    "    \n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    \n",
    "    \n",
    "    \n",
    "    epoch = 0\n",
    "    batch_size = 100\n",
    "    \n",
    "    \n",
    "    while epoch < 2:\n",
    "        step = 1\n",
    "        train_X, test_X, train_y, test_y = train_test_split(trainX, train_Y_mod, test_size=0.20, random_state=random.randint(1,99))\n",
    "        # Keep training until reach max iterations\n",
    "        for k in range(int(len(train_X)/batch_size)):\n",
    "#             print(\"Range: \", k*batch_size, \" to \", (k+1)*batch_size)\n",
    "            \n",
    "            batch_x, batch_y = train_X[k*batch_size : (k+1)*batch_size], train_y[k*batch_size : (k+1)*batch_size]\n",
    "    \n",
    "            train_step.run(feed_dict={x: batch_x, y_: batch_y, keep_prob: 0.6})\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_: batch_y, keep_prob: 1.0})\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print(\"Epoch: %d, step %d, training accuracy %g\"%(epoch, k, train_accuracy*100))\n",
    "            \n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "        acc = 0.0\n",
    "        for k in range(int(len(test_X)/batch_size)):\n",
    "            batch_x, batch_y = test_X[k*batch_size : (k+1)*batch_size], test_y[k*batch_size : (k+1)*batch_size]\n",
    "            acc += sess.run(accuracy, feed_dict={x: batch_x, y_: batch_y, keep_prob: 1.0})\n",
    "        \n",
    "        print(\" test accuracy, \", str(100*acc/int(len(test_X)/batch_size)))\n",
    "        \n",
    "        epoch += 1\n",
    "            \n",
    "    \n",
    "    np.save('./weights/W_conv1', sess.run(W_conv1))\n",
    "    np.save('./weights/W_conv2', sess.run(W_conv2))\n",
    "    np.save('./weights/W_fc1', sess.run(W_fc1))\n",
    "    np.save('./weights/W_fc2', sess.run(W_fc2))\n",
    "    \n",
    "    np.save('./weights/b_conv1', sess.run(b_conv1))\n",
    "    np.save('./weights/b_conv2', sess.run(b_conv2))\n",
    "    np.save('./weights/b_fc1', sess.run(b_fc1))\n",
    "    np.save('./weights/b_fc2', sess.run(b_fc2))\n",
    "\n",
    "    sess.close()\n",
    "    \n",
    "    \n",
    "def test_cnn_web(testX):\n",
    "    \n",
    "    download = False\n",
    "    if download:\n",
    "        downloadData_CNN()\n",
    "    \n",
    "    W_conv1 = np.load('./weights/W_conv1.npy')\n",
    "    b_conv1 = np.load('./weights/b_conv1.npy')\n",
    "\n",
    "    W_conv2 = np.load('./weights/W_conv2.npy')\n",
    "    b_conv2 = np.load('./weights/b_conv2.npy')\n",
    "\n",
    "    W_fc1 = np.load('./weights/W_fc1.npy')\n",
    "    b_fc1 = np.load('./weights/b_fc1.npy')\n",
    "\n",
    "    W_fc2 = np.load('./weights/W_fc2.npy')\n",
    "    b_fc2 = np.load('./weights/b_fc2.npy')\n",
    "        \n",
    "    x = tf.placeholder(tf.float32, [None,28,28,1])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    \n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "        \n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    \n",
    "    output = []\n",
    "    batch_size = 100\n",
    "    \n",
    "    for k in range((int(len(testX)/batch_size))):\n",
    "        out = sess.run(y_conv, feed_dict={x: testX[k*batch_size : (k+1)*batch_size], keep_prob: 1.0})\n",
    "        output += list(sess.run(tf.arg_max(out,1)))\n",
    "        \n",
    "        \n",
    "    sess.close()\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    trainX, trainY, testX, testY = load_mnist()\n",
    "    print(\"Shapes: \", trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "\n",
    "#     print(\"\\nDigit sample\")\n",
    "#     print_digit(trainX[1], trainY[1])\n",
    "\n",
    "#     downloadData()\n",
    "    \n",
    "#     train_nn(trainX, trainY)\n",
    "#     labels = test_nn(testX)\n",
    "#     accuracy = np.mean((labels == testY)) * 100.0\n",
    "#     print(\"\\nDNN Test accuracy: %lf%%\" % accuracy)\n",
    "\n",
    "\n",
    "    train_cnn_web(trainX, trainY)\n",
    "    labels = test_cnn_web(testX)\n",
    "    accuracy = np.mean((labels == testY)) * 100.0\n",
    "    print(\"\\nCNN Test accuracy: %lf%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  (60000, 28, 28, 1) (60000,) (10000, 28, 28, 1) (10000,)\n",
      "Epoch: 0, step 99, training accuracy 95\n",
      "Epoch: 0, step 199, training accuracy 97\n",
      "Epoch: 0, step 299, training accuracy 99\n",
      "Epoch: 0, step 399, training accuracy 99\n",
      " test accuracy,  98.208331267\n",
      "Epoch: 1, step 99, training accuracy 100\n",
      "Epoch: 1, step 199, training accuracy 99\n",
      "Epoch: 1, step 299, training accuracy 98\n",
      "Epoch: 1, step 399, training accuracy 98\n",
      " test accuracy,  98.7666645646\n",
      "\n",
      "CNN Test accuracy: 98.590000%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
